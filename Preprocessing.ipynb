{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import pickle\n",
    "from os import path\n",
    "from time import sleep, time\n",
    "from datetime import datetime\n",
    "from shapely.geometry import Point\n",
    "from difflib import get_close_matches\n",
    "from geopy.geocoders import Nominatim\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading sample.tsv file...\n",
      "is done!\n"
     ]
    }
   ],
   "source": [
    "# Read the csv file\n",
    "print('Reading sample.tsv file...')\n",
    "df = pd.read_csv(\n",
    "    path.join('data', 'sample.tsv'),\n",
    "    sep=\"\\t\",\n",
    "    encoding='utf-8',\n",
    "    escapechar='\\\\',\n",
    "    na_values='N',\n",
    "    quoting=csv.QUOTE_NONE,\n",
    "    header=None\n",
    ")\n",
    "print('is done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading schema.txt file...\n",
      "is done!\n"
     ]
    }
   ],
   "source": [
    "print('Reading schema.txt file...')\n",
    "schema = pd.read_csv(\n",
    "    path.join('data', 'schema.txt'),\n",
    "    sep=\"\\s+\",\n",
    "    header =None\n",
    ")\n",
    "print('is done!')\n",
    "# Rename the dataframe columns\n",
    "df.columns = schema[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# From what we observe it is better to consider latitude/logitude columns for the tweets which these\n",
    "# information are available.\n",
    "df['latitude'].fillna(df['placeLatitude'], inplace=True)\n",
    "df['longitude'].fillna(df['placeLongitude'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Drop rows with NaN values in important columns\n",
    "df.dropna(\n",
    "    subset=['createdAt', 'id', 'latitude', 'longitude', 'userId'],\n",
    "    how='any',\n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Change the string in 'createdAt' column to datetime format\n",
    "df['createdAt'] = pd.to_datetime(\n",
    "    df['createdAt'],\n",
    "    format='%Y-%m-%d %H:%M:%S',\n",
    "    errors='coerce'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove duplicated tweets with the same id\n",
    "if not df['id'].is_unique:\n",
    "    df.drop_duplicates(subset='id', inplace=True)\n",
    "# Remove unnecessary columns\n",
    "df = df[['id', 'userId', 'createdAt', 'text', 'longitude', 'latitude']]\n",
    "# Reset index\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to recover the cities location from latitude-longitude pairs, we use two different strategies:\n",
    "\n",
    "1. **online strategy:** we use the geopy API to send a request containing information about the longitude and latitude of a place. The main cumbersome here is that all these kind of online APIs have some kind of request rate limit, and as it is suggested in [its website](http://wiki.openstreetmap.org/wiki/Nominatim_usage_policy), the time between two consecutive request should be more that 1 seconds. This actually makes the online approach so slow. One remedy to accelerate the process is to save longitude-latitude: location pair to a dictionary. Thus, before sending a request, we first check whether we have the location in our dictionary or not.\n",
    "\n",
    "2. **offline strategy:** we can also use the geojson or topojson files for Switzerland and its neighbor countries. The corresponding geofiles are downloaded from the following github repositories:\n",
    "    1. Switzerland topojson file from [swiss_map repo](https://github.com/interactivethings/swiss-maps).\n",
    "    2. France geojson file from [france-geojson repo](https://github.com/gregoiredavid/france-geojson).\n",
    "    3. Italy geojson file from [leaflet-geojson-selector repo](https://github.com/stefanocudini/leaflet-geojson-selector).\n",
    "    4. Germany geojson file from [deutschlandGeoJSON repo](https://github.com/isellsoap/deutschlandGeoJSON)\n",
    "    5. Austria geojson file from [click_that_hood repo](https://github.com/codeforamerica/click_that_hood).\n",
    "    6. Liechtenstein geojson file from [CountryGeoJSONCollection repo](https://github.com/LonnyGomes/CountryGeoJSONCollection).\n",
    "\n",
    "In general, for the offinle strategy, one can also follow this [stackoverflow response](http://stackoverflow.com/questions/6159074/given-the-lat-long-coordinates-how-can-we-find-out-the-city-country/6355183#6355183) or this [one](http://stackoverflow.com/a/24871449/5267664). The first one relies on the geoname database while the second one actually gives us a procedure to find geojson files for any country."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Online strategy:\n",
    "We start from the online strategy. The following function find the country together with its state/canton of a location. We will see later that we could do the same thing in the offline approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function for finding a location from the latitude-longitude information using online API\n",
    "geolocator = Nominatim()\n",
    "locations = dict()\n",
    "def online_locating(data):\n",
    "    lat = str(data.latitude)\n",
    "    lng = str(data.longitude)\n",
    "    lookup = ','.join([lat, lng])\n",
    "    if lookup not in set(locations.keys()):\n",
    "        try:\n",
    "            location = geolocator.reverse(lookup, language='en')\n",
    "        except TimeOut:\n",
    "            online_locating(data)                \n",
    "        try:\n",
    "            country = location.raw['address']['country_code'].upper()\n",
    "        except:\n",
    "            country = float('NaN')\n",
    "        try:\n",
    "            state = location.raw['address']['state']\n",
    "        except:\n",
    "            try:\n",
    "                state = location.raw['address']['country']\n",
    "            except:\n",
    "                state = float('NaN')\n",
    "        locations[lookup] = {'country': country, 'state': state}\n",
    "        sleep(1) # sleep for 1 sec (required by Nominatim usage policy)\n",
    "    return pd.Series({'country': locations[lookup]['country'],\n",
    "                      'state': locations[lookup]['state']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time is 1962.436 seconds.\n"
     ]
    }
   ],
   "source": [
    "online_df = df.copy()\n",
    "t = time()\n",
    "online_df[['country', 'state']] = online_df.apply(lambda x: online_locating(x), axis=1)\n",
    "elapsed = time() - t\n",
    "print('Elapsed time is ' + str(round(elapsed, 4)) + ' seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, it took very long to even recover locations for the sample file. Hence, it does not make sense to follow the online approach for the actual problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Offline approach:\n",
    "As we mentioned before, it is necessary to download the required geojson/topojson file to run the offline approach. All files are available in data/geofiles folder. In this part, we use [gepandas](http://geopandas.org/) for furthur analysis. The resulting dataframes can be used to find the location of tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ch_gdf = gpd.read_file(path.join('data/geofiles', 'ch-cantons.json'))\n",
    "fr_gdf = gpd.read_file(path.join('data/geofiles', 'france-states.geojson'))\n",
    "it_gdf = gpd.read_file(path.join('data/geofiles', 'italy-states.json'))\n",
    "de_gdf = gpd.read_file(path.join('data/geofiles', 'germany-states.geojson'))\n",
    "at_gdf = gpd.read_file(path.join('data/geofiles', 'austria-states.geojson'))\n",
    "li_gdf = gpd.read_file(path.join('data/geofiles', 'liechtenstein.geojson'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Modify dataframes for merging\n",
    "ch_gdf = ch_gdf[['geometry', 'name']]\n",
    "ch_gdf['country'] = 'CH'\n",
    "\n",
    "fr_gdf = fr_gdf[['geometry', 'name']]\n",
    "fr_gdf['country'] = 'FR'\n",
    "\n",
    "it_gdf = it_gdf[['geometry', 'name']]\n",
    "it_gdf['country'] = 'IT'\n",
    "\n",
    "de_gdf = de_gdf[['geometry', 'NAME_1']]\n",
    "de_gdf = de_gdf.rename(columns={'NAME_1': 'name'})\n",
    "de_gdf['country'] = 'DE'\n",
    "\n",
    "at_gdf = at_gdf[['geometry', 'name']]\n",
    "at_gdf['country'] = 'AT'\n",
    "\n",
    "li_gdf = li_gdf[['geometry', 'NAME']]\n",
    "li_gdf = li_gdf.rename(columns={'NAME': 'name'})\n",
    "li_gdf['country'] = 'LI'\n",
    "\n",
    "gdf_poly = pd.concat([ch_gdf, fr_gdf, it_gdf, de_gdf, at_gdf, li_gdf], ignore_index=True)\n",
    "gdf_poly = gdf_poly.rename(columns={'name': 'state'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The R-tree structure in geopandas dataframe enables us to find the twitters location very fast. The following function find each location is inside which state/canton. A tutorial to show how to take advantages of R-tree structure is available [here](http://geoffboeing.com/2016/10/r-tree-spatial-index-python/#more-2183)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gdf_point = gpd.GeoDataFrame(df)\n",
    "gdf_point['geometry'] = df.apply(lambda row: Point(row.longitude, row.latitude), axis=1)\n",
    "gdf_point.crs = gdf_poly.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time is 2.206 seconds.\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "offline_gdf = gpd.tools.sjoin(gdf_point, gdf_poly, how=\"left\")\n",
    "elapsed = time() - t\n",
    "print('Elapsed time is ' + str(round(elapsed, 4)) + ' seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "offline_gdf.drop_duplicates(subset='id', inplace=True)\n",
    "offline_gdf.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there are still some tweets which we are not able to find its location. For these tweets, we call the online_locating function in order to find the location of the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "null_index = offline_gdf['state'].isnull()\n",
    "offline_gdf.loc[null_index,['country', 'state']] = offline_gdf[null_index].apply(\n",
    "    lambda row: online_locating(row), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There also some inconsistansy between the geolacating by offline approach and online approach. The following function aims to remove these inconsistansies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def modify_dataframe(row):\n",
    "    countries = set(gdf_poly['country'].unique())\n",
    "    if row['country'] not in countries:\n",
    "        row['state'] = row['country']\n",
    "    else:\n",
    "        sub_gdf = gdf_poly[gdf_poly['country'] == row['country']]\n",
    "        states = sub_gdf.state.values\n",
    "        if 'Bavaria' in row['state']:\n",
    "            row['state'] = 'Bayern'\n",
    "        elif row['state'] == 'Great East':\n",
    "            row['state'] = 'Alsace-Champagne-Ardenne-Lorraine'\n",
    "        elif row['state'] == 'Grisons':\n",
    "            row['state'] = 'Graub√ºnden'\n",
    "        elif row['state'] == 'Aosta Valley':\n",
    "            row['state'] = \"valle d'aosta\"\n",
    "        else:\n",
    "            row['state'] = get_close_matches(row['state'], states, 1, 0)[0]\n",
    "        row['index_right'] = sub_gdf[sub_gdf['state'] == row['state']].index.values[0]\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "offline_gdf.loc[null_index,:] = offline_gdf[null_index].apply(\n",
    "    lambda row: modify_dataframe(row), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can measure the difference between online/exact and offline/approximate approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The error in locating country is %0.49\n",
      "The error in locating state is %0.77\n"
     ]
    }
   ],
   "source": [
    "online_df = online_df.apply(lambda row: modify_dataframe(row), axis=1)\n",
    "print('The error in locating country is %' + str(\n",
    "        round(100 * sum(online_df['country'] != offline_gdf['country'])/online_df.shape[0],2)))\n",
    "print('The error in locating state is %' + str(\n",
    "        round(100 * sum(online_df['state'] != offline_gdf['state'])/online_df.shape[0],2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the error is not significant. Thus, the proposed mixed offline-online mapping is both fast and accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove tweets from countries which are not in our list\n",
    "offline_gdf = offline_gdf[offline_gdf['index_right'].notnull()]\n",
    "# Reset index\n",
    "offline_gdf.reset_index(drop=True, inplace=True)\n",
    "# Convert float number in the index_right column to integer\n",
    "offline_gdf['index_right'] = offline_gdf['index_right'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save the results with pickle\n",
    "with open('data/cache/' + 'offline_gdf' + '.pkl', 'wb') as f:\n",
    "    pickle.dump(offline_gdf, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
