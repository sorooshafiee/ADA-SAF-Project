{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import csv\n",
    "import pickle\n",
    "from os import path\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Settings\n",
    "DIR_DATA = path.join('data', 'twitter data')\n",
    "threshold_tweets = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing a file splitter in order to load the huge tsv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We need a file splitter\n",
    "def tsv_splitter(file_path):\n",
    "    splitLen = 10**4       # 1e4 lines per file same as the sample file\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf8') as input_f:\n",
    "        count = 0\n",
    "        at = 0\n",
    "        dest = None\n",
    "        row = ''\n",
    "        for line in input_f:\n",
    "            if count % splitLen == 0:\n",
    "                if line.count('\\t') != 19:\n",
    "                    count -= 1\n",
    "                else:\n",
    "                    if dest:\n",
    "                        dest.close()\n",
    "                    replacement = '.' + str(at) + '.'\n",
    "                    dest = open(replacement.join(file_path.rsplit('.', 1)),\n",
    "                                'w',\n",
    "                                newline='\\n',\n",
    "                                encoding='utf8')\n",
    "                    at += 1\n",
    "            dest.write(line)\n",
    "            count += 1            \n",
    "    dest.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split twex.tsv file\n",
    "tsv_splitter(path.join(DIR_DATA, 'twex.tsv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twex.tsv file...\n",
      "is done!\n"
     ]
    }
   ],
   "source": [
    "# Read the splitted tsv files\n",
    "all_files = glob(path.join(DIR_DATA, '*.tsv'))\n",
    "if path.join(DIR_DATA, 'twex.tsv') in all_files:\n",
    "    all_files.remove(path.join(DIR_DATA, 'twex.tsv'))\n",
    "\n",
    "df_from_each_file = (pd.read_csv(\n",
    "    file_name,\n",
    "    sep=\"\\t\",\n",
    "    encoding='utf-8',\n",
    "    escapechar='\\\\',\n",
    "    na_values='N',\n",
    "    quoting=csv.QUOTE_NONE,\n",
    "    header=None\n",
    "    )\n",
    "    for file_name in all_files)\n",
    "print('Reading twex.tsv file...')\n",
    "df = pd.concat(df_from_each_file, ignore_index=True)\n",
    "print('is done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading schema.txt file...\n",
      "is done!\n"
     ]
    }
   ],
   "source": [
    "# Read the schema file\n",
    "print('Reading schema.txt file...')\n",
    "schema = pd.read_csv(\n",
    "    path.join(DIR_DATA, 'schema.txt'),\n",
    "    sep=\"\\s+\",\n",
    "    header=None\n",
    ")\n",
    "print('is done!')\n",
    "\n",
    "# Rename the dataframe columns\n",
    "df.columns = schema[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Our observations suggest that considering latitude/logitude columns is more accurate\n",
    "df['latitude'].fillna(df['placeLatitude'], inplace=True)\n",
    "df['longitude'].fillna(df['placeLongitude'], inplace=True)\n",
    "\n",
    "# Just keep the important columns\n",
    "df = df[['id', 'userId', 'createdAt', 'longitude', 'latitude', 'text']]\n",
    "\n",
    "# Change the string in 'createdAt' column to datetime format\n",
    "df['createdAt'] = pd.to_datetime(\n",
    "    df['createdAt'],\n",
    "    format='%Y-%m-%d %H:%M:%S',\n",
    "    errors='coerce'\n",
    ")\n",
    "\n",
    "# Change the possible strings to numbers\n",
    "df['id'] = df['id'].apply(lambda x: pd.to_numeric(x, errors='coerce'))\n",
    "df['userId'] = df['userId'].apply(lambda x: pd.to_numeric(x, errors='coerce'))\n",
    "df['longitude'] = df['longitude'].apply(lambda x: pd.to_numeric(x, errors='coerce'))\n",
    "df['latitude'] = df['latitude'].apply(lambda x: pd.to_numeric(x, errors='coerce'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop rows with NaN values in important columns\n",
    "df = df.dropna(subset=['id', 'userId', 'createdAt', 'longitude', 'latitude'], how='any')\n",
    "\n",
    "# Change the id and user id format to integer \n",
    "df['id'] = df['id'].astype(np.int64)\n",
    "df['userId'] = df['userId'].astype(np.int64)\n",
    "\n",
    "# Remove duplicated tweets with the same id (it is too time consuming!)\n",
    "df = df.drop_duplicates(subset='id')\n",
    "\n",
    "# Reset index\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add some columns for further analysis\n",
    "df['day'] = df['createdAt'].map(lambda x: x.day)\n",
    "df['month'] = df['createdAt'].map(lambda x: x.month)\n",
    "df['year'] = df['createdAt'].map(lambda x: x.year)\n",
    "daily_user = ['userId', 'year', 'month', 'day']\n",
    "df['daily_tweets'] = df.groupby(by=daily_user)['userId'].transform('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove rows corresponding to people who have less than a threshold value in one day\n",
    "df = df[df['daily_tweets'] >= threshold_tweets].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the results with pickle\n",
    "with open(path.join(DIR_DATA, 'clean_data.pkl'), 'wb') as in_file:\n",
    "    pickle.dump(df, in_file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
