{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "from os import path\n",
    "from time import sleep, time\n",
    "from datetime import datetime\n",
    "from shapely.geometry import Point\n",
    "from difflib import get_close_matches\n",
    "from geopy.geocoders import Nominatim\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read the csv file\n",
    "print('Reading sample.tsv file...')\n",
    "df = pd.read_csv(\n",
    "    path.join('data', 'sample.tsv'),\n",
    "    sep=\"\\t\",\n",
    "    encoding='utf-8',\n",
    "    escapechar='\\\\',\n",
    "    na_values='N',\n",
    "    quoting=csv.QUOTE_NONE,\n",
    "    header=None\n",
    ")\n",
    "print('is done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Reading schema.txt file...')\n",
    "schema = pd.read_csv(\n",
    "    path.join('data', 'schema.txt'),\n",
    "    sep=\"\\s+\",\n",
    "    header =None\n",
    ")\n",
    "print('is done!')\n",
    "# Rename the dataframe columns\n",
    "df.columns = schema[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Drop rows with NaN values in important columns\n",
    "df.dropna(\n",
    "    subset=['createdAt', 'id', 'placeLatitude', 'placeLongitude', 'userId'],\n",
    "    how='any',\n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Change the string in 'createdAt' column to datetime format\n",
    "df['createdAt'] = pd.to_datetime(\n",
    "    df['createdAt'],\n",
    "    format='%Y-%m-%d %H:%M:%S',\n",
    "    errors='coerce'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove useless columns\n",
    "df = df[['id', 'userId', 'createdAt','placeLatitude','placeLongitude']]\n",
    "# Remove duplicated tweets with the same id\n",
    "if not df['id'].is_unique:\n",
    "    df.drop_duplicates(subset='id', inplace=True)\n",
    "# Reset index\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to recover the cities location from latitude-longitude pairs, we use two different strategies:\n",
    "\n",
    "1. **online strategy:** we use the geopy API to send a request containing information about the longitude and latitude of a place. The main cumbersome here is that all these kind of online APIs have some kind of request rate limit, and as it is suggested in [its website](http://wiki.openstreetmap.org/wiki/Nominatim_usage_policy), the time between two consecutive request should be more that 1 seconds. This actually makes the online approach so slow. One remedy to accelerate the process is to save longitude-latitude: location pair to a dictionary. Thus, before sending a request, we first check whether we have the location in our dictionary or not.\n",
    "\n",
    "2. **offline strategy:** we can also use the geojson or topojson files for Switzerland and its neighbor countries. The corresponding geofiles are downloaded from the following github repositories:\n",
    "    1. Switzerland topojson file from [swiss_map repo](https://github.com/d-qn/swiss-maps).\n",
    "    2. France geojson file from [france-geojson repo](https://github.com/gregoiredavid/france-geojson).\n",
    "    3. Italy geojson file from [leaflet-geojson-selector repo](https://github.com/stefanocudini/leaflet-geojson-selector).\n",
    "    4. Germany geojson file from [deutschlandGeoJSON repo](https://github.com/isellsoap/deutschlandGeoJSON)\n",
    "    5. Austria geojson file from [click_that_hood repo](https://github.com/codeforamerica/click_that_hood).\n",
    "    6. Liechtenstein geojson file from [CountryGeoJSONCollection repo](https://github.com/LonnyGomes/CountryGeoJSONCollection).\n",
    "\n",
    "In general, for the offinle strategy, one can also follow this [stackoverflow response](http://stackoverflow.com/questions/6159074/given-the-lat-long-coordinates-how-can-we-find-out-the-city-country/6355183#6355183) or this [one](http://stackoverflow.com/a/24871449/5267664). The first one relies on the geoname database while the second one actually gives us a procedure to find geojson files for any country."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Online strategy:\n",
    "We start from the online strategy. The following function find the country together with its state/canton of a location. We will see later that we could do the same thing in the offline approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function for finding a location from the latitude-longitude information using online API\n",
    "geolocator = Nominatim()\n",
    "locations = dict()\n",
    "def online_locating(data):\n",
    "    lat = str(data.placeLatitude)\n",
    "    lng = str(data.placeLongitude)\n",
    "    lookup = ','.join([lat, lng])\n",
    "    if lookup not in set(locations.keys()):\n",
    "        location = geolocator.reverse(lookup, language='en')        \n",
    "        try:\n",
    "            country = location.raw['address']['country_code'].upper()\n",
    "        except:\n",
    "            country = float('NaN')\n",
    "        try:\n",
    "            state = location.raw['address']['state']\n",
    "        except:\n",
    "            try:\n",
    "                state = location.raw['address']['country']\n",
    "            except:\n",
    "                state = float('NaN')\n",
    "        locations[lookup] = {'country': country, 'state': state}\n",
    "        sleep(1) # sleep for 1 sec (required by Nominatim usage policy)\n",
    "    return pd.Series({'country': locations[lookup]['country'],\n",
    "                      'state': locations[lookup]['state']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t = time()\n",
    "df[['country', 'state']] = df.apply(lambda x: online_locating(x), axis=1)\n",
    "elapsed = time() - t\n",
    "print('Elapsed time is ' + str(round(elapsed, 4)) + ' seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, it took very long to even recover locations for the sample file. Hence, it does not make sense to follow the online approach for the actual problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Offline approach:\n",
    "As we mentioned before, it is necessary to download the required geojson/topojson file to run the offline approach. All files are available in data/geofiles folder. In this part, we use [gepandas](http://geopandas.org/) for furthur analysis. The resulting dataframes can be used to find the location of tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ch_gdf = gpd.read_file(path.join('data/geofiles', 'ch-cantons.json'))\n",
    "fr_gdf = gpd.read_file(path.join('data/geofiles', 'france-states.geojson'))\n",
    "it_gdf = gpd.read_file(path.join('data/geofiles', 'italy-states.json'))\n",
    "de_gdf = gpd.read_file(path.join('data/geofiles', 'germany-states.geojson'))\n",
    "at_gdf = gpd.read_file(path.join('data/geofiles', 'austria-states.geojson'))\n",
    "li_gdf = gpd.read_file(path.join('data/geofiles', 'liechtenstein.geojson'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Modify dataframes for merging\n",
    "ch_gdf = ch_gdf[['geometry', 'name']]\n",
    "ch_gdf['country'] = 'CH'\n",
    "\n",
    "fr_gdf = fr_gdf[['geometry', 'name']]\n",
    "fr_gdf['country'] = 'FR'\n",
    "\n",
    "it_gdf = it_gdf[['geometry', 'name']]\n",
    "it_gdf['country'] = 'IT'\n",
    "\n",
    "de_gdf = de_gdf[['geometry', 'NAME_1']]\n",
    "de_gdf = de_gdf.rename(columns={'NAME_1': 'name'})\n",
    "de_gdf['country'] = 'DE'\n",
    "\n",
    "at_gdf = at_gdf[['geometry', 'name']]\n",
    "at_gdf['country'] = 'AT'\n",
    "\n",
    "li_gdf = li_gdf[['geometry', 'NAME']]\n",
    "li_gdf = li_gdf.rename(columns={'NAME': 'name'})\n",
    "li_gdf['country'] = 'LI'\n",
    "\n",
    "gdf_poly = pd.concat([ch_gdf, fr_gdf, it_gdf, de_gdf, at_gdf, li_gdf], ignore_index=True)\n",
    "gdf_poly = gdf_poly.rename(columns={'name': 'state'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The R-tree structure in geopandas dataframe enables us to find the twitters location very fast. The following function find each location is inside which state/canton. A tutorial to show how to take advantages of R-tree structure is available [here](http://geoffboeing.com/2016/10/r-tree-spatial-index-python/#more-2183)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gdf_point = gpd.GeoDataFrame(df)\n",
    "gdf_point['geometry'] = df.apply(lambda row: Point(row.placeLongitude, row.placeLatitude), axis=1)\n",
    "gdf_point.crs = gdf_poly.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t = time()\n",
    "gdf_join = gpd.tools.sjoin(gdf_point, gdf_poly, how=\"left\")\n",
    "elapsed = time() - t\n",
    "print('Elapsed time is ' + str(round(elapsed, 4)) + ' seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gdf_join.drop_duplicates(subset='id', inplace=True)\n",
    "gdf_join.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "null_index = gdf_join['state'].isnull()\n",
    "gdf_join.loc[null_index,['country', 'state']] = gdf_join[null_index].apply(\n",
    "    lambda row: online_locating(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def modify_dataframe(row):\n",
    "    countries = set(gdf_poly['country'].unique())\n",
    "    if row['country'] not in countries:\n",
    "        row['state'] = row['country']\n",
    "    else:\n",
    "        sub_gdf = gdf_poly[gdf_poly['country'] == row['country']]\n",
    "        indices = sub_gdf.index\n",
    "        states = sub_gdf.state.values\n",
    "        if 'Bavaria' in row['state']:\n",
    "            row['state'] = 'Bayern'\n",
    "        else:\n",
    "            row['state'] = get_close_matches(row['state'], states, 1, 0)[0]\n",
    "        row['index_right'] = sub_gdf[sub_gdf['state'] == row['state']].index.values[0]\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gdf_join.loc[null_index,:] = gdf_join[null_index].apply(\n",
    "    lambda row: modify_dataframe(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove tweets from countries which are not in our list\n",
    "gdf_join = gdf_join[gdf_join['index_right'].notnull()]\n",
    "# Reset index\n",
    "gdf_join.reset_index(drop=True, inplace=True)\n",
    "# Convert float number in the index_right column to integer\n",
    "gdf_join['index_right'] = gdf_join['index_right'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
