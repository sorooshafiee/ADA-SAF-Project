{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pickle\n",
    "from os import path\n",
    "from time import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from geopy.distance import vincenty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Settings\n",
    "DIR_DATA = path.join('data', 'twitter data')\n",
    "DIR_GEO = path.join('data', 'geofiles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loading the saved file is as easy as running these lines of code\n",
    "with open(path.join(DIR_DATA, 'clean_data.pkl'), 'rb') as in_file:\n",
    "    df = pickle.load(in_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_denoising(sub_df, crt_speed=60):\n",
    "    \"\"\" This function aims to identify noisy tweets. By the term noisy, we mean that the\n",
    "    reported location for the tweet is noisy.\"\"\"\n",
    "    zipped_columns = sub_df.tolist()\n",
    "    lst = list(zip(*zipped_columns))\n",
    "    lat = lst[0]\n",
    "    lng = lst[1]\n",
    "    tw_time = lst[2]\n",
    "    denoised = [True] * len(sub_df)\n",
    "    points = list(zip(lat, lng))\n",
    "\n",
    "    orig = points[0:-2]\n",
    "    dest1 = points[1:-1]\n",
    "    dest2 = points[2::]\n",
    "\n",
    "    for index in range(len(orig)):\n",
    "        d1 = vincenty(dest1[index], orig[index]).meters\n",
    "        t1 = tw_time[index+1] - tw_time[index]\n",
    "        t1 = t1.total_seconds()\n",
    "        v1 = d1 / t1 if t1 else float('inf')\n",
    "\n",
    "        d2 = vincenty(dest2[index], dest1[index]).meters\n",
    "        t2 = tw_time[index+2] - tw_time[index+1]\n",
    "        t2 = t2.total_seconds()\n",
    "        v2 = d2 / t2 if t2 else float('inf')\n",
    "\n",
    "        d3 = vincenty(dest2[index], orig[index]).meters\n",
    "        t3 = tw_time[index+2] - tw_time[index]\n",
    "        t3 = t3.total_seconds()\n",
    "        v3 = d3 / t3 if t3 else float('inf')\n",
    "\n",
    "        if np.isinf(v1) | np.isinf(v2) | np.isinf(v3):\n",
    "            denoised = [False] * len(denoised)\n",
    "            break\n",
    "        if (v1 > crt_speed) & (v2 > crt_speed):\n",
    "            if v3 <= crt_speed:\n",
    "                denoised[index+1] = False\n",
    "            else:\n",
    "                denoised[index] = False\n",
    "                denoised[index+1] = False\n",
    "                if index == len(orig) - 1:\n",
    "                    denoised[index+2] = False\n",
    "        if (v1 > crt_speed) & (v2 <= crt_speed):\n",
    "            denoised[index] = False\n",
    "\n",
    "    return denoised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove noisy tweets with the above function\n",
    "daily_user = ['userId', 'year', 'month', 'day']\n",
    "df['new'] = tuple(zip(df['latitude'], df['longitude'], df['createdAt']))\n",
    "not_noisy = df.groupby(by=daily_user)['new'].transform(lambda x: data_denoising(x))\n",
    "df = df[not_noisy].reset_index(drop=True)\n",
    "# Remove the generated column\n",
    "del df['new']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load geofiles\n",
    "ch_gdf = gpd.read_file(path.join(DIR_GEO, 'ch-cantons.json'))\n",
    "fr_gdf = gpd.read_file(path.join(DIR_GEO, 'france-states.geojson'))\n",
    "it_gdf = gpd.read_file(path.join(DIR_GEO, 'italy-states.json'))\n",
    "de_gdf = gpd.read_file(path.join(DIR_GEO, 'germany-states.geojson'))\n",
    "at_gdf = gpd.read_file(path.join(DIR_GEO, 'austria-states.geojson'))\n",
    "li_gdf = gpd.read_file(path.join(DIR_GEO, 'liechtenstein.geojson'))\n",
    "\n",
    "# Modify dataframes for merging\n",
    "ch_gdf = ch_gdf[['geometry', 'name']]\n",
    "ch_gdf['country'] = 'CH'\n",
    "\n",
    "fr_gdf = fr_gdf[['geometry', 'name']]\n",
    "fr_gdf['country'] = 'FR'\n",
    "\n",
    "it_gdf = it_gdf[['geometry', 'name']]\n",
    "it_gdf['country'] = 'IT'\n",
    "\n",
    "de_gdf = de_gdf[['geometry', 'NAME_1']]\n",
    "de_gdf = de_gdf.rename(columns={'NAME_1': 'name'})\n",
    "de_gdf['country'] = 'DE'\n",
    "\n",
    "at_gdf = at_gdf[['geometry', 'name']]\n",
    "at_gdf['country'] = 'AT'\n",
    "\n",
    "li_gdf = li_gdf[['geometry', 'NAME']]\n",
    "li_gdf = li_gdf.rename(columns={'NAME': 'name'})\n",
    "li_gdf['country'] = 'LI'\n",
    "# Concatinate the dataframes\n",
    "df_poly = pd.concat([ch_gdf, fr_gdf, it_gdf, de_gdf, at_gdf, li_gdf], ignore_index=True)\n",
    "df_poly = df_poly.rename(columns={'name': 'state'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert our dataframe to a geopandas dataframe\n",
    "df = gpd.GeoDataFrame(df)\n",
    "df['geometry'] = df.apply(lambda row: Point(row.longitude, row.latitude), axis=1)\n",
    "df.crs = df_poly.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start spatial merging process...\n",
      "Elapsed time is 3795.78 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Offline locating using spatial indexing in geopandas\n",
    "print('Start spatial merging process...')\n",
    "t = time()\n",
    "df = gpd.tools.sjoin(df, df_poly, how=\"left\")\n",
    "elapsed = time() - t\n",
    "print('Elapsed time is ' + str(round(elapsed, 2)) + ' seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove unlocated tweets\n",
    "df = df[df['state'].notnull()].reset_index(drop=True)\n",
    "# Remove index_right column\n",
    "del df['index_right']\n",
    "# Drop duplicate tweets. It might be possible that we locate boundaries into two different cantons\n",
    "# df = df.drop_duplicates(subset='id')\n",
    "# df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_path(sub_df, cmt_num = 10):       \n",
    "    origin = sub_df.values[0:-1]\n",
    "    destination = sub_df.values[1::]\n",
    "    index = origin != destination\n",
    "    path = float('NaN')\n",
    "    if (index.any()) & (sum(index) <= cmt_num):\n",
    "        path = '->'.join(origin[index])\n",
    "        path = '->'.join([path, destination[index][-1]])\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "daily_patterns = df.groupby(by=daily_user)['state'].apply(lambda x: find_path(x))\n",
    "daily_patterns = daily_patterns.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the results with pickle\n",
    "with open(path.join('data', 'daily_patterns.pkl'), 'wb') as in_file:\n",
    "    pickle.dump(daily_patterns, in_file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
