{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pickle\n",
    "from os import path\n",
    "from time import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Settings\n",
    "DIR_DATA = path.join('data', 'twitter data')\n",
    "DIR_GEO = path.join('data', 'geofiles')\n",
    "threshold_tweets = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# We need a file splitter\n",
    "splitLen = 10**4       # 1e4 lines per file same as the sample file\n",
    "\n",
    "with open(path.join(DIR_DATA, 'twex.tsv'), 'r', encoding='utf8') as input_f:\n",
    "    count = 0\n",
    "    at = 0\n",
    "    dest = None\n",
    "    row = ''\n",
    "    for line in input_f:\n",
    "        if count % splitLen == 0:\n",
    "            if line.count('\\t') != 19:\n",
    "                count -= 1\n",
    "            else:\n",
    "                if dest:\n",
    "                    dest.close()\n",
    "                dest = open(path.join(DIR_DATA, 'twex.' + str(at) + '.tsv'),\n",
    "                            'w',\n",
    "                            newline='\\n',\n",
    "                            encoding='utf8')\n",
    "                at += 1\n",
    "        dest.write(line)\n",
    "        count += 1            \n",
    "dest.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read the tsv files\n",
    "all_files = glob(path.join(DIR_DATA, '*.tsv'))\n",
    "if path.join(DIR_DATA, 'twex.tsv') in all_files:\n",
    "    all_files.remove(path.join(DIR_DATA, 'twex.tsv'))\n",
    "\n",
    "df_from_each_file = (pd.read_csv(\n",
    "    file_name,\n",
    "    sep=\"\\t\",\n",
    "    encoding='utf-8',\n",
    "    escapechar='\\\\',\n",
    "    na_values='N',\n",
    "    quoting=csv.QUOTE_NONE,\n",
    "    header=None\n",
    "    )\n",
    "    for file_name in all_files)\n",
    "print('Reading twex.tsv file...')\n",
    "df = pd.concat(df_from_each_file, ignore_index=True)\n",
    "print('is done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read the schema file\n",
    "print('Reading schema.txt file...')\n",
    "schema = pd.read_csv(\n",
    "    path.join(DIR_DATA, 'schema.txt'),\n",
    "    sep=\"\\s+\",\n",
    "    header=None\n",
    ")\n",
    "print('is done!')\n",
    "\n",
    "# Rename the dataframe columns\n",
    "df.columns = schema[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Our observations suggest that considering latitude/logitude columns is more accurate\n",
    "df['latitude'].fillna(df['placeLatitude'], inplace=True)\n",
    "df['longitude'].fillna(df['placeLongitude'], inplace=True)\n",
    "\n",
    "# Just keep the important columns\n",
    "df = df[['id', 'userId', 'createdAt', 'longitude', 'latitude', 'text']]\n",
    "\n",
    "# Change the string in 'createdAt' column to datetime format\n",
    "df['createdAt'] = pd.to_datetime(\n",
    "    df['createdAt'],\n",
    "    format='%Y-%m-%d %H:%M:%S',\n",
    "    errors='coerce'\n",
    ")\n",
    "\n",
    "# Change the possible strings to numbers\n",
    "df['id'] = df['id'].apply(lambda x: pd.to_numeric(x, errors='coerce'))\n",
    "df['id'] = df['id'].astype(int)\n",
    "df['userId'] = df['userId'].apply(lambda x: pd.to_numeric(x, errors='coerce'))\n",
    "df['userId'] = df['userId'].astype(int)\n",
    "df['longitude'] = df['longitude'].apply(lambda x: pd.to_numeric(x, errors='coerce'))\n",
    "df['latitude'] = df['latitude'].apply(lambda x: pd.to_numeric(x, errors='coerce'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Drop rows with NaN values in important columns\n",
    "df = df.dropna(subset=['id', 'userId', 'createdAt', 'longitude', 'latitude'], how='any')\n",
    "\n",
    "# Remove duplicated tweets with the same id (it is too time consuming!)\n",
    "# df = df.drop_duplicates(subset='id')\n",
    "\n",
    "# Reset index\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add some columns for further analysis\n",
    "df['day'] = df['createdAt'].map(lambda x: x.day)\n",
    "df['month'] = df['createdAt'].map(lambda x: x.month)\n",
    "df['year'] = df['createdAt'].map(lambda x: x.year)\n",
    "daily_user = ['userId', 'year', 'month', 'day']\n",
    "df['daily_tweets'] = df.groupby(by=daily_user)['userId'].transform('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove rows corresponding to people who have less than a threshold value in one day\n",
    "threshold_tweets = 10\n",
    "df = df[df['daily_tweets'] >= threshold_tweets].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove noisy tweets with the above function\n",
    "df['new'] = tuple(zip(df['latitude'], df['longitude'], df['createdAt']))\n",
    "not_noisy = df.groupby(by=daily_user)['new'].transform(lambda x: data_denoising(x))\n",
    "df = df[not_noisy].reset_index(drop=True)\n",
    "# Remove the generated column\n",
    "del df['new']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load geofiles\n",
    "ch_gdf = gpd.read_file(path.join(DIR_GEO, 'ch-cantons.json'))\n",
    "fr_gdf = gpd.read_file(path.join(DIR_GEO, 'france-states.geojson'))\n",
    "it_gdf = gpd.read_file(path.join(DIR_GEO, 'italy-states.json'))\n",
    "de_gdf = gpd.read_file(path.join(DIR_GEO, 'germany-states.geojson'))\n",
    "at_gdf = gpd.read_file(path.join(DIR_GEO, 'austria-states.geojson'))\n",
    "li_gdf = gpd.read_file(path.join(DIR_GEO, 'liechtenstein.geojson'))\n",
    "\n",
    "# Modify dataframes for merging\n",
    "ch_gdf = ch_gdf[['geometry', 'name']]\n",
    "ch_gdf['country'] = 'CH'\n",
    "\n",
    "fr_gdf = fr_gdf[['geometry', 'name']]\n",
    "fr_gdf['country'] = 'FR'\n",
    "\n",
    "it_gdf = it_gdf[['geometry', 'name']]\n",
    "it_gdf['country'] = 'IT'\n",
    "\n",
    "de_gdf = de_gdf[['geometry', 'NAME_1']]\n",
    "de_gdf = de_gdf.rename(columns={'NAME_1': 'name'})\n",
    "de_gdf['country'] = 'DE'\n",
    "\n",
    "at_gdf = at_gdf[['geometry', 'name']]\n",
    "at_gdf['country'] = 'AT'\n",
    "\n",
    "li_gdf = li_gdf[['geometry', 'NAME']]\n",
    "li_gdf = li_gdf.rename(columns={'NAME': 'name'})\n",
    "li_gdf['country'] = 'LI'\n",
    "# Concatinate the dataframes\n",
    "df_poly = pd.concat([ch_gdf, fr_gdf, it_gdf, de_gdf, at_gdf, li_gdf], ignore_index=True)\n",
    "df_poly = df_poly.rename(columns={'name': 'state'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert our dataframe to a geopandas dataframe\n",
    "df = gpd.GeoDataFrame(df)\n",
    "df['geometry'] = df.apply(lambda row: Point(row.longitude, row.latitude), axis=1)\n",
    "df.crs = df_poly.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Offline locating using spatial indexing in geopandas\n",
    "print('Start spatial merging process...')\n",
    "t = time()\n",
    "df = gpd.tools.sjoin(df, df_poly, how=\"left\")\n",
    "elapsed = time() - t\n",
    "print('Elapsed time is ' + str(round(elapsed, 2)) + ' seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove unlocated tweets\n",
    "df = df[df['state'].notnull()].reset_index(drop=True)\n",
    "# Remove index_right column\n",
    "del df['index_right']\n",
    "# Drop duplicate tweets. It might be possible that we locate boundaries into two different cantons\n",
    "# df = df.drop_duplicates(subset='id')\n",
    "# df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_path(sub_df, cmt_num = 10):       \n",
    "    origin = sub_df.values[0:-1]\n",
    "    destination = sub_df.values[1::]\n",
    "    index = origin != destination\n",
    "    path = float('NaN')\n",
    "    if (index.any()) & (sum(index) <= cmt_num):\n",
    "        path = '->'.join(origin[index])\n",
    "        path = '->'.join([path, destination[index][-1]])\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "daily_patterns = df.groupby(by=daily_user)['state'].apply(lambda x: find_path(x))\n",
    "daily_patterns = daily_patterns.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the results with pickle\n",
    "with open(path.join('data', 'daily_patterns.pkl'), 'wb') as in_file:\n",
    "    pickle.dump(daily_patterns, in_file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(df.groupby(by=daily_user))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "daily_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
