{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Our observations suggest that considering latitude/logitude columns is more accurate\n",
    "df['latitude'].fillna(df['placeLatitude'], inplace=True)\n",
    "df['longitude'].fillna(df['placeLongitude'], inplace=True)\n",
    "\n",
    "# Drop rows with NaN values in important columns\n",
    "df.dropna(\n",
    "    subset=['id', 'userId', 'createdAt', 'longitude', 'latitude'],\n",
    "    how='any',\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "\n",
    "# Change the string in 'createdAt' column to datetime format\n",
    "df['createdAt'] = pd.to_datetime(\n",
    "    df['createdAt'],\n",
    "    format='%Y-%m-%d %H:%M:%S',\n",
    "    errors='coerce'\n",
    ")\n",
    "\n",
    "# Remove duplicated tweets with the same id\n",
    "if not df['id'].is_unique:\n",
    "    df.drop_duplicates(subset='id', inplace=True)\n",
    "\n",
    "# Remove unnecessary columns\n",
    "df = df[['id', 'userId', 'createdAt', 'longitude', 'latitude']]\n",
    "\n",
    "# Add some columns for further analysis\n",
    "df['day'] = df['createdAt'].map(lambda x: x.day)\n",
    "df['month'] = df['createdAt'].map(lambda x: x.month)\n",
    "df['year'] = df['createdAt'].map(lambda x: x.year)\n",
    "daily_user = ['userId', 'day', 'month', 'year']\n",
    "df['daily_tweets'] = df.groupby(by=daily_user)['userId'].transform('count')\n",
    "df['new'] = tuple(zip(df['latitude'], df['longitude'], df['createdAt']))\n",
    "\n",
    "# Remove rows corresponding to people who have less than a threshold value in one day\n",
    "df = df[df['daily_tweets'] >= threshold_tweets].reset_index(drop=True)\n",
    "\n",
    "# Function for removing noisy tweets\n",
    "def data_denoising(sub_df, crt_speed=60):\n",
    "    \"\"\" This function aims to identify noisy tweets. By the term noisy, we mean that the\n",
    "    reported location for the tweet is noisy.\"\"\"\n",
    "    zipped_columns = sub_df.tolist()\n",
    "    lst = list(zip(*zipped_columns))\n",
    "    lat = lst[0]\n",
    "    lng = lst[1]\n",
    "    tw_time = lst[2]\n",
    "    states = lst[3]\n",
    "    points = list(zip(lat, lng))\n",
    "\n",
    "    if len(sub_df) == 2:\n",
    "        d = vincenty(points[1], points[0]).meters\n",
    "        t = tw_time[1] - tw_time[0]\n",
    "        t = t.total_seconds()\n",
    "        v = d / t if t else float('inf')\n",
    "        if v > crt_speed:\n",
    "            return [float('NaN'), float('NaN')]\n",
    "        else:\n",
    "            return states\n",
    "    else:\n",
    "        orig = points[0:-2]\n",
    "        dest1 = points[1:-1]\n",
    "        dest2 = points[2::]\n",
    "        denoised = list(states)\n",
    "\n",
    "        for index in range(len(orig)):\n",
    "            d1 = vincenty(dest1[index], orig[index]).meters\n",
    "            t1 = tw_time[index+1] - tw_time[index]\n",
    "            t1 = t1.total_seconds()\n",
    "            v1 = d1 / t1 if t1 else float('inf')\n",
    "\n",
    "            d2 = vincenty(dest2[index], dest1[index]).meters\n",
    "            t2 = tw_time[index+2] - tw_time[index+1]\n",
    "            t2 = t2.total_seconds()\n",
    "            v2 = d2 / t2 if t2 else float('inf')\n",
    "\n",
    "            d3 = vincenty(dest2[index], orig[index]).meters\n",
    "            t3 = tw_time[index+2] - tw_time[index]\n",
    "            t3 = t3.total_seconds()\n",
    "            v3 = d3 / t3 if t3 else float('inf')\n",
    "\n",
    "            if np.isinf(v1) | np.isinf(v2) | np.isinf(v3):\n",
    "                denoised = [float('NaN')] * len(denoised)\n",
    "                break\n",
    "            if (v1 > crt_speed) & (v2 > crt_speed):\n",
    "                if v3 <= crt_speed:\n",
    "                    denoised[index+1] = float('NaN')\n",
    "                else:\n",
    "                    denoised[index] = float('NaN')\n",
    "                    denoised[index+1] = float('NaN')\n",
    "                    if index == len(orig) - 1:\n",
    "                        denoised[index+2] = float('NaN')\n",
    "            if (v1 > crt_speed) & (v2 <= crt_speed):\n",
    "                denoised[index] = float('NaN')\n",
    "\n",
    "        return denoised\n",
    "\n",
    "# Remove noisy tweets with the above function\n",
    "df['state'] = df.groupby(by=daily_user)['new'].transform(lambda x: data_denoising(x))\n",
    "df = df[df['state'].notnull()].reset_index(drop=True)\n",
    "\n",
    "# Remove the new column\n",
    "del df['new']\n",
    "\n",
    "# Load geofiles\n",
    "ch_gdf = gpd.read_file(path.join(DIR_GEO, 'ch-cantons.json'))\n",
    "fr_gdf = gpd.read_file(path.join(DIR_GEO, 'france-states.geojson'))\n",
    "it_gdf = gpd.read_file(path.join(DIR_GEO, 'italy-states.json'))\n",
    "de_gdf = gpd.read_file(path.join(DIR_GEO, 'germany-states.geojson'))\n",
    "at_gdf = gpd.read_file(path.join(DIR_GEO, 'austria-states.geojson'))\n",
    "li_gdf = gpd.read_file(path.join(DIR_GEO, 'liechtenstein.geojson'))\n",
    "\n",
    "# Modify dataframes for merging\n",
    "ch_gdf = ch_gdf[['geometry', 'name']]\n",
    "ch_gdf['country'] = 'CH'\n",
    "\n",
    "fr_gdf = fr_gdf[['geometry', 'name']]\n",
    "fr_gdf['country'] = 'FR'\n",
    "\n",
    "it_gdf = it_gdf[['geometry', 'name']]\n",
    "it_gdf['country'] = 'IT'\n",
    "\n",
    "de_gdf = de_gdf[['geometry', 'NAME_1']]\n",
    "de_gdf = de_gdf.rename(columns={'NAME_1': 'name'})\n",
    "de_gdf['country'] = 'DE'\n",
    "\n",
    "at_gdf = at_gdf[['geometry', 'name']]\n",
    "at_gdf['country'] = 'AT'\n",
    "\n",
    "li_gdf = li_gdf[['geometry', 'NAME']]\n",
    "li_gdf = li_gdf.rename(columns={'NAME': 'name'})\n",
    "li_gdf['country'] = 'LI'\n",
    "# Concatinate the dataframes\n",
    "df_poly = pd.concat([ch_gdf, fr_gdf, it_gdf, de_gdf, at_gdf, li_gdf], ignore_index=True)\n",
    "df_poly = df_poly.rename(columns={'name': 'state'})\n",
    "\n",
    "# Convert our dataframe to a geopandas dataframe\n",
    "df = gpd.GeoDataFrame(df)\n",
    "df['geometry'] = df.apply(lambda row: Point(row.longitude, row.latitude), axis=1)\n",
    "df.crs = df_poly.crs\n",
    "\n",
    "# Offline locating using spatial indexing in geopandas\n",
    "print('Start spatial merging process...')\n",
    "t = time()\n",
    "df = gpd.tools.sjoin(df, df_poly, how=\"left\")\n",
    "elapsed = time() - t\n",
    "print('Elapsed time is ' + str(round(elapsed, 4)) + ' seconds.')\n",
    "\n",
    "# Drop duplicate tweets. It might be possible that we locate boundaries into two different cantons\n",
    "df = df.drop_duplicates(subset='id')\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Find unlocated tweets\n",
    "null_index = df['state'].isnull()\n",
    "\n",
    "# Initialization for online locating\n",
    "geolocator = Nominatim()\n",
    "locations = dict()\n",
    "\n",
    "# Online locating function for missing data\n",
    "def online_locating(data):\n",
    "    \"\"\" This function find the state and country of a location using an online API\"\"\"\n",
    "    lat = str(data.latitude)\n",
    "    lng = str(data.longitude)\n",
    "    lookup = ','.join([lat, lng])\n",
    "    if lookup not in set(locations.keys()):\n",
    "        try:\n",
    "            location = geolocator.reverse(lookup, language='en')\n",
    "        except TimeOut:\n",
    "            online_locating(data)                \n",
    "        try:\n",
    "            country = location.raw['address']['country_code'].upper()\n",
    "        except:\n",
    "            country = float('NaN')\n",
    "        try:\n",
    "            state = location.raw['address']['state']\n",
    "        except:\n",
    "            try:\n",
    "                state = location.raw['address']['country']\n",
    "            except:\n",
    "                state = float('NaN')\n",
    "        locations[lookup] = {'country': country, 'state': state}\n",
    "        sleep(1) # sleep for 1 sec (required by Nominatim usage policy)\n",
    "    return pd.Series({'country': locations[lookup]['country'],\n",
    "                      'state': locations[lookup]['state']})\n",
    "\n",
    "# Function for finding a location from the latitude-longitude information using online API\n",
    "df.loc[null_index, ['country', 'state']] = df[null_index].apply(\n",
    "    lambda row: online_locating(row), axis=1)\n",
    "\n",
    "# Function to modify the online API results\n",
    "def modify_dataframe(row):\n",
    "    countries = set(df_poly['country'].unique())\n",
    "    if row['country'] not in countries:\n",
    "        row['state'] = row['country']\n",
    "    else:\n",
    "        sub_gdf = df_poly[df_poly['country'] == row['country']]\n",
    "        states = sub_gdf.state.values\n",
    "        if 'Bavaria' in row['state']:\n",
    "            row['state'] = 'Bayern'\n",
    "        elif row['state'] == 'Great East':\n",
    "            row['state'] = 'Alsace-Champagne-Ardenne-Lorraine'\n",
    "        elif row['state'] == 'Grisons':\n",
    "            row['state'] = 'GraubÃ¼nden'\n",
    "        elif row['state'] == 'Aosta Valley':\n",
    "            row['state'] = \"valle d'aosta\"\n",
    "        else:\n",
    "            row['state'] = get_close_matches(row['state'], states, 1, 0)[0]\n",
    "        row['index_right'] = sub_gdf[sub_gdf['state'] == row['state']].index.values[0]\n",
    "    return row\n",
    "\n",
    "# Apply function modifier to the dataframe\n",
    "df.loc[null_index,:] = df[null_index].apply(\n",
    "    lambda row: modify_dataframe(row), axis=1)\n",
    "\n",
    "# Remove tweets from countries which are not in our list\n",
    "df = df[df['index_right'].notnull()]\n",
    "\n",
    "# Reset index\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Remove index_right column\n",
    "del df['index_right']\n",
    "\n",
    "\n",
    "def find_path(sub_df, cmt_num = 10):       \n",
    "    origin = sub_df.values[0:-1]\n",
    "    destination = sub_df.values[1::]\n",
    "    index = origin != destination\n",
    "    path = float('NaN')\n",
    "    if (index.any()) & (sum(index) <= cmt_num):\n",
    "        path = '->'.join(origin[index])\n",
    "        path = '->'.join([path, destination[index][-1]])\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
