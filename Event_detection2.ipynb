{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to a study by Pear Analytics [16], about 40% of all the tweets are pointless “babbles” like “have to get something from the minimart downstairs”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from os import path\n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import nltk\n",
    "import csv\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import CMUTweetTagger\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "import scipy.cluster.hierarchy as sch\n",
    "import fastcluster\n",
    "from collections import Counter\n",
    "import codecs\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import sys\n",
    "import langid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DIR_DATA = path.join('data', 'twitter data1')\n",
    "DIR_GEO = path.join('data', 'geofiles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loading the saved file is as easy as running these lines of code\n",
    "#with open(path.join(DIR_DATA, 'clean_data.pkl'), 'rb') as in_file:\n",
    "#    df = pickle.load(in_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twex.tsv file...\n",
      "is done!\n"
     ]
    }
   ],
   "source": [
    "# Read the splitted tsv files\n",
    "all_files = glob(path.join(DIR_DATA, '*.tsv'))\n",
    "if path.join(DIR_DATA, 'twex.tsv') in all_files:\n",
    "    all_files.remove(path.join(DIR_DATA, 'twex.tsv'))\n",
    "\n",
    "df_from_each_file = (pd.read_csv(\n",
    "    file_name,\n",
    "    sep=\"\\t\",\n",
    "    encoding='utf-8',\n",
    "    escapechar='\\\\',\n",
    "    na_values='N',\n",
    "    quoting=csv.QUOTE_NONE,\n",
    "    header=None\n",
    "    )\n",
    "    for file_name in all_files)\n",
    "print('Reading twex.tsv file...')\n",
    "df = pd.concat(df_from_each_file, ignore_index=True)\n",
    "print('is done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading schema.txt file...\n",
      "is done!\n"
     ]
    }
   ],
   "source": [
    "# Read the schema file\n",
    "print('Reading schema.txt file...')\n",
    "schema = pd.read_csv(\n",
    "    path.join(DIR_DATA, 'schema.txt'),\n",
    "    sep=\"\\s+\",\n",
    "    header=None\n",
    ")\n",
    "print('is done!')\n",
    "\n",
    "# Rename the dataframe columns\n",
    "df.columns = schema[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Our observations suggest that considering latitude/logitude columns is more accurate\n",
    "df['latitude'].fillna(df['placeLatitude'], inplace=True)\n",
    "df['longitude'].fillna(df['placeLongitude'], inplace=True)\n",
    "\n",
    "# Just keep the important columns\n",
    "df = df[['id', 'userId', 'createdAt', 'longitude', 'latitude', 'text']]\n",
    "\n",
    "# Change the string in 'createdAt' column to datetime format\n",
    "df['createdAt'] = pd.to_datetime(\n",
    "    df['createdAt'],\n",
    "    format='%Y-%m-%d %H:%M:%S',\n",
    "    errors='coerce'\n",
    ")\n",
    "\n",
    "# Change the possible strings to numbers\n",
    "df['id'] = df['id'].apply(lambda x: pd.to_numeric(x, errors='coerce'))\n",
    "df['userId'] = df['userId'].apply(lambda x: pd.to_numeric(x, errors='coerce'))\n",
    "df['longitude'] = df['longitude'].apply(lambda x: pd.to_numeric(x, errors='coerce'))\n",
    "df['latitude'] = df['latitude'].apply(lambda x: pd.to_numeric(x, errors='coerce'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop rows with NaN values in important columns\n",
    "df = df.dropna(subset=['id', 'userId', 'createdAt', 'longitude', 'latitude'], how='any')\n",
    "\n",
    "# Change the id and user id format to integer \n",
    "df['id'] = df['id'].astype(np.int64)\n",
    "df['userId'] = df['userId'].astype(np.int64)\n",
    "\n",
    "# Remove duplicated tweets with the same id (it is too time consuming!)\n",
    "df = df.drop_duplicates(subset='id')\n",
    "\n",
    "# Reset index\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add some columns for further analysis\n",
    "df['day'] = df['createdAt'].map(lambda x: x.day)\n",
    "df['month'] = df['createdAt'].map(lambda x: x.month)\n",
    "df['year'] = df['createdAt'].map(lambda x: x.year)\n",
    "daily_user = ['userId', 'year', 'month', 'day']\n",
    "df['daily_tweets'] = df.groupby(by=daily_user)['userId'].transform('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.sort_values(by='createdAt', ascending=1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Here we normalize the text, the code is taken from \n",
    "#https://github.com/heerme/twitter-topics/blob/master/twitter-topics-from-json-text-stream.py\n",
    "def normalize_text(text):\n",
    "    if type(text) is not str:\n",
    "        print(text)\n",
    "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+)|(pic\\.twitter\\.com/[^\\s]+))','', text)\n",
    "    text = re.sub('@[^\\s]+','', text)\n",
    "    text = re.sub('#([^\\s]+)', '', text)\n",
    "    text = re.sub('[:;>?<=*+()/,\\-#!$%\\{˜|\\}\\[^_\\\\@\\]1234567890’‘]',' ', text)\n",
    "    text = re.sub('[\\d]','', text)\n",
    "    text = text.replace(\".\", '')\n",
    "    text = text.replace(\"'\", ' ')\n",
    "    text = text.replace(\"\\\"\", ' ')\n",
    "    #text = text.replace(\"-\", \" \")\n",
    "    #normalize some utf8 encoding\n",
    "    text = text.replace(\"\\x9d\",' ').replace(\"\\x8c\",' ')\n",
    "    text = text.replace(\"\\xa0\",' ')\n",
    "    text = text.replace(\"\\x9d\\x92\", ' ').replace(\"\\x9a\\xaa\\xf0\\x9f\\x94\\xb5\", ' ').replace(\"\\xf0\\x9f\\x91\\x8d\\x87\\xba\\xf0\\x9f\\x87\\xb8\", ' ').replace(\"\\x9f\",' ').replace(\"\\x91\\x8d\",' ')\n",
    "    text = text.replace(\"\\xf0\\x9f\\x87\\xba\\xf0\\x9f\\x87\\xb8\",' ').replace(\"\\xf0\",' ').replace('\\xf0x9f','').replace(\"\\x9f\\x91\\x8d\",' ').replace(\"\\x87\\xba\\x87\\xb8\",' ')\t\n",
    "    text = text.replace(\"\\xe2\\x80\\x94\",' ').replace(\"\\x9d\\xa4\",' ').replace(\"\\x96\\x91\",' ').replace(\"\\xe1\\x91\\xac\\xc9\\x8c\\xce\\x90\\xc8\\xbb\\xef\\xbb\\x89\\xd4\\xbc\\xef\\xbb\\x89\\xc5\\xa0\\xc5\\xa0\\xc2\\xb8\",' ')\n",
    "    text = text.replace(\"\\xe2\\x80\\x99s\", \" \").replace(\"\\xe2\\x80\\x98\", ' ').replace(\"\\xe2\\x80\\x99\", ' ').replace(\"\\xe2\\x80\\x9c\", \" \").replace(\"\\xe2\\x80\\x9d\", \" \")\n",
    "    text = text.replace(\"\\xe2\\x82\\xac\", \" \").replace(\"\\xc2\\xa3\", \" \").replace(\"\\xc2\\xa0\", \" \").replace(\"\\xc2\\xab\", \" \").replace(\"\\xf0\\x9f\\x94\\xb4\", \" \").replace(\"\\xf0\\x9f\\x87\\xba\\xf0\\x9f\\x87\\xb8\\xf0\\x9f\", \"\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find the hashtags and users\n",
    "df['Hashtags'] = df['text'].apply(lambda x:{tag.strip(\"#\") for tag in x.split() if tag.startswith(\"#\")})\n",
    "df['users'] = df['text'].apply(lambda x:{tag.strip(\"@\") for tag in x.split() if tag.startswith(\"@\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.dropna(subset = ['text'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['processed_text'] = df['text'].apply(lambda x: normalize_text(x))\n",
    "df.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  filter the blank cells\n",
    "filter_text = (df[\"processed_text\"] != \"\") & (df[\"processed_text\"] != \" \") & (df[\"processed_text\"] != \"  \") \\\n",
    "    & (df[\"processed_text\"] != \"   \") \n",
    "df = df[filter_text]\n",
    "df.reset_index(inplace=True,drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filtering = df['year'] == 2016\n",
    "df2016 = df[filtering]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2016.reset_index(drop = True,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# As there are so many tweets about the weather, we postprocess the data by removing the tweets with removing them\n",
    "Weather_words = ['falling', 'CForecast','humidity','pressure','wind','temperature ', 'Conditions', 'Cloudy', 'hpa', 'humidite', 'info',\\\n",
    "                 'km','Rain','fog','foggy','cloudy','kmh', 'mm', 'pluie', 'pression', 'temp','Temp', 'vent']\n",
    "filtering = df2016['processed_text'].apply(lambda x:not any(w in Weather_words for w in x.split()))\n",
    "df2016_noW = df2016[filtering]\n",
    "df2016_noW.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "stop_words.extend(nltk.corpus.stopwords.words('french'))\n",
    "stop_words.extend(nltk.corpus.stopwords.words('italian'))\n",
    "stop_words.extend(nltk.corpus.stopwords.words('german'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to our search in the literutre we found that stemming is not that helpful, in the case one want ot do stemming, first he should detect the language (for example by langdetect package) and then use the stemmer developed for that language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#not super pythonic, no, not at all.\n",
    "#use extend so it's a big flat list of vocab\n",
    "totalvocab_tokenized = []\n",
    "for index,row in df2016_noW.iloc[:300000].iterrows():\n",
    "    tex = row['processed_text']\n",
    "    allwords_tokenized = tokenize_only(tex)\n",
    "    totalvocab_tokenized.extend(allwords_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The code is taken from\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "def _calculate_languages_ratios(text):\n",
    "    \"\"\"\n",
    "    Calculate probability of given text to be written in several languages and\n",
    "    return a dictionary that looks like {'french': 2, 'spanish': 4, 'english': 0}\n",
    "    \n",
    "    @param text: Text whose language want to be detected\n",
    "    @type text: str\n",
    "    \n",
    "    @return: Dictionary with languages and unique stopwords seen in analyzed text\n",
    "    @rtype: dict\n",
    "    \"\"\"\n",
    "\n",
    "    languages_ratios = {}\n",
    "\n",
    "    '''\n",
    "    nltk.wordpunct_tokenize() splits all punctuations into separate tokens\n",
    "    \n",
    "    >>> wordpunct_tokenize(\"That's thirty minutes away. I'll be there in ten.\")\n",
    "    ['That', \"'\", 's', 'thirty', 'minutes', 'away', '.', 'I', \"'\", 'll', 'be', 'there', 'in', 'ten', '.']\n",
    "    '''\n",
    "\n",
    "    tokens = wordpunct_tokenize(text)\n",
    "    words = [word.lower() for word in tokens]\n",
    "\n",
    "    # Compute per language included in nltk number of unique stopwords appearing in analyzed text\n",
    "    for language in stopwords.fileids():\n",
    "        stopwords_set = set(stopwords.words(language))\n",
    "        words_set = set(words)\n",
    "        common_elements = words_set.intersection(stopwords_set)\n",
    "\n",
    "        languages_ratios[language] = len(common_elements) # language \"score\"\n",
    "\n",
    "    return languages_ratios\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "def detect_language(text):\n",
    "    \"\"\"\n",
    "    Calculate probability of given text to be written in several languages and\n",
    "    return the highest scored.\n",
    "    \n",
    "    It uses a stopwords based approach, counting how many unique stopwords\n",
    "    are seen in analyzed text.\n",
    "    \n",
    "    @param text: Text whose language want to be detected\n",
    "    @type text: str\n",
    "    \n",
    "    @return: Most scored language guessed\n",
    "    @rtype: str\n",
    "    \"\"\"\n",
    "\n",
    "    ratios = _calculate_languages_ratios(text)\n",
    "\n",
    "    most_rated_language = max(ratios, key=ratios.get)\n",
    "\n",
    "    return most_rated_language\n",
    "\n",
    "\n",
    "\n",
    "    language = detect_language(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# finding the language of the texts\n",
    "df2016_noW['language']= df2016_noW['processed_text'].apply(lambda x: langid.classify(x))\n",
    "df2016_noW['language'] = df2016_noW['language'].apply(lambda x:x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Frequency of the words\n",
    "x = df2016_noW.groupby('language').count()\n",
    "test = x.sort('id', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtering = df2016_noW['language'] == 'en'\n",
    "df2016_noW_eng = df2016_noW[filtering]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min, sys: 723 ms, total: 1min\n",
      "Wall time: 1min 1s\n",
      "(300000, 32)\n"
     ]
    }
   ],
   "source": [
    "# As the \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#define vectorizer parameters\n",
    "tfidf_vectorizer = TfidfVectorizer( max_features=200000,\n",
    "                                 min_df= 0.01, stop_words='english',\n",
    "                                 use_idf=True, tokenizer=tokenize_only, ngram_range=(1,3))\n",
    "\n",
    "%time tfidf_matrix = tfidf_vectorizer.fit_transform(df2016_noW_eng.iloc[:300000]['processed_text']) #fit the vectorizer to synopses\n",
    "\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "terms = tfidf_vectorizer.get_feature_names() # some possible features for being a stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "dist = 1 - cosine_similarity(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amp',\n",
       " 'best',\n",
       " 'd',\n",
       " 'day',\n",
       " 'don',\n",
       " 'don t',\n",
       " 'geneva',\n",
       " 'good',\n",
       " 'great',\n",
       " 'happy',\n",
       " 'just',\n",
       " 'just posted',\n",
       " 'know',\n",
       " 'like',\n",
       " 'love',\n",
       " 'm',\n",
       " 'morning',\n",
       " 'nan',\n",
       " 'need',\n",
       " 'new',\n",
       " 'people',\n",
       " 'photo',\n",
       " 'posted',\n",
       " 's',\n",
       " 'switzerland',\n",
       " 't',\n",
       " 'thank',\n",
       " 'thanks',\n",
       " 'time',\n",
       " 'today',\n",
       " 'want',\n",
       " 'world']"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
