{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to a study by Pear Analytics [16], about 40% of all the tweets are pointless “babbles” like “have to get something from the minimart downstairs”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from os import path\n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import nltk\n",
    "import csv\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import CMUTweetTagger\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "import scipy.cluster.hierarchy as sch\n",
    "import fastcluster\n",
    "from collections import Counter\n",
    "import codecs\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import sys\n",
    "import langid\n",
    "from guess_language import guess_language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DIR_DATA = path.join('data', 'twitter data1')\n",
    "DIR_GEO = path.join('data', 'geofiles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loading the saved file is as easy as running these lines of code\n",
    "#with open(path.join(DIR_DATA, 'clean_data.pkl'), 'rb') as in_file:\n",
    "#    df = pickle.load(in_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read the splitted tsv files\n",
    "all_files = glob(path.join(DIR_DATA, '*.tsv'))\n",
    "if path.join(DIR_DATA, 'twex.tsv') in all_files:\n",
    "    all_files.remove(path.join(DIR_DATA, 'twex.tsv'))\n",
    "\n",
    "df_from_each_file = (pd.read_csv(\n",
    "    file_name,\n",
    "    sep=\"\\t\",\n",
    "    encoding='utf-8',\n",
    "    escapechar='\\\\',\n",
    "    na_values='N',\n",
    "    quoting=csv.QUOTE_NONE,\n",
    "    header=None\n",
    "    )\n",
    "    for file_name in all_files)\n",
    "print('Reading twex.tsv file...')\n",
    "df = pd.concat(df_from_each_file, ignore_index=True)\n",
    "print('is done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read the schema file\n",
    "print('Reading schema.txt file...')\n",
    "schema = pd.read_csv(\n",
    "    path.join(DIR_DATA, 'schema.txt'),\n",
    "    sep=\"\\s+\",\n",
    "    header=None\n",
    ")\n",
    "print('is done!')\n",
    "\n",
    "# Rename the dataframe columns\n",
    "df.columns = schema[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_temp = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.iloc[:300000]\n",
    "df.reset_index(drop=True,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Our observations suggest that considering latitude/logitude columns is more accurate\n",
    "df['latitude'].fillna(df['placeLatitude'], inplace=True)\n",
    "df['longitude'].fillna(df['placeLongitude'], inplace=True)\n",
    "\n",
    "# Just keep the important columns\n",
    "df = df[['id', 'userId', 'createdAt', 'longitude', 'latitude', 'text']]\n",
    "\n",
    "# Change the string in 'createdAt' column to datetime format\n",
    "df['createdAt'] = pd.to_datetime(\n",
    "    df['createdAt'],\n",
    "    format='%Y-%m-%d %H:%M:%S',\n",
    "    errors='coerce'\n",
    ")\n",
    "\n",
    "# Change the possible strings to numbers\n",
    "df['id'] = df['id'].apply(lambda x: pd.to_numeric(x, errors='coerce'))\n",
    "df['userId'] = df['userId'].apply(lambda x: pd.to_numeric(x, errors='coerce'))\n",
    "df['longitude'] = df['longitude'].apply(lambda x: pd.to_numeric(x, errors='coerce'))\n",
    "df['latitude'] = df['latitude'].apply(lambda x: pd.to_numeric(x, errors='coerce'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df['language'] = np.nan\n",
    "df.loc[:,'language'] =  df['text'].apply(lambda x: langid.classify(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_temp = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df[df['language']=='en']\n",
    "df.reset_index(drop=True,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop rows with NaN values in important columns\n",
    "df = df.dropna(subset=['id', 'userId', 'createdAt', 'longitude', 'latitude'], how='any')\n",
    "\n",
    "# Change the id and user id format to integer \n",
    "df['id'] = df['id'].astype(np.int64)\n",
    "df['userId'] = df['userId'].astype(np.int64)\n",
    "\n",
    "# Remove duplicated tweets with the same id (it is too time consuming!)\n",
    "df = df.drop_duplicates(subset='id')\n",
    "\n",
    "# Reset index\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add some columns for further analysis\n",
    "df['day'] = df['createdAt'].map(lambda x: x.day)\n",
    "df['month'] = df['createdAt'].map(lambda x: x.month)\n",
    "df['year'] = df['createdAt'].map(lambda x: x.year)\n",
    "daily_user = ['userId', 'year', 'month', 'day']\n",
    "df['daily_tweets'] = df.groupby(by=daily_user)['userId'].transform('count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Here we normalize the text, the code is taken from \n",
    "#https://github.com/heerme/twitter-topics/blob/master/twitter-topics-from-json-text-stream.py\n",
    "def normalize_text(text):\n",
    "    if type(text) is not str:\n",
    "        print(text)\n",
    "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+)|(pic\\.twitter\\.com/[^\\s]+))','', text)\n",
    "    text = re.sub('@[^\\s]+','', text)\n",
    "    text = re.sub('#([^\\s]+)', '', text)\n",
    "    text = re.sub('[:;>?<=*+()/,\\-#!$%\\{˜|\\}\\[^_\\\\@\\]1234567890’‘]',' ', text)\n",
    "    text = re.sub('[\\d]','', text)\n",
    "    text = text.replace(\".\", '')\n",
    "    text = text.replace(\"'\", ' ')\n",
    "    text = text.replace(\"\\\"\", ' ')\n",
    "    #text = text.replace(\"-\", \" \")\n",
    "    #normalize some utf8 encoding\n",
    "    text = text.replace(\"\\x9d\",' ').replace(\"\\x8c\",' ')\n",
    "    text = text.replace(\"\\xa0\",' ')\n",
    "    text = text.replace(\"\\x9d\\x92\", ' ').replace(\"\\x9a\\xaa\\xf0\\x9f\\x94\\xb5\", ' ').replace(\"\\xf0\\x9f\\x91\\x8d\\x87\\xba\\xf0\\x9f\\x87\\xb8\", ' ').replace(\"\\x9f\",' ').replace(\"\\x91\\x8d\",' ')\n",
    "    text = text.replace(\"\\xf0\\x9f\\x87\\xba\\xf0\\x9f\\x87\\xb8\",' ').replace(\"\\xf0\",' ').replace('\\xf0x9f','').replace(\"\\x9f\\x91\\x8d\",' ').replace(\"\\x87\\xba\\x87\\xb8\",' ')\t\n",
    "    text = text.replace(\"\\xe2\\x80\\x94\",' ').replace(\"\\x9d\\xa4\",' ').replace(\"\\x96\\x91\",' ').replace(\"\\xe1\\x91\\xac\\xc9\\x8c\\xce\\x90\\xc8\\xbb\\xef\\xbb\\x89\\xd4\\xbc\\xef\\xbb\\x89\\xc5\\xa0\\xc5\\xa0\\xc2\\xb8\",' ')\n",
    "    text = text.replace(\"\\xe2\\x80\\x99s\", \" \").replace(\"\\xe2\\x80\\x98\", ' ').replace(\"\\xe2\\x80\\x99\", ' ').replace(\"\\xe2\\x80\\x9c\", \" \").replace(\"\\xe2\\x80\\x9d\", \" \")\n",
    "    text = text.replace(\"\\xe2\\x82\\xac\", \" \").replace(\"\\xc2\\xa3\", \" \").replace(\"\\xc2\\xa0\", \" \").replace(\"\\xc2\\xab\", \" \").replace(\"\\xf0\\x9f\\x94\\xb4\", \" \").replace(\"\\xf0\\x9f\\x87\\xba\\xf0\\x9f\\x87\\xb8\\xf0\\x9f\", \"\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find the hashtags and users\n",
    "df['Hashtags'] = df['text'].apply(lambda x:{tag.strip(\"#\") for tag in x.split() if tag.startswith(\"#\")})\n",
    "df['users'] = df['text'].apply(lambda x:{tag.strip(\"@\") for tag in x.split() if tag.startswith(\"@\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.dropna(subset = ['text'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['processed_text'] = df['text'].apply(lambda x: normalize_text(x))\n",
    "df.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  filter the blank cells\n",
    "filter_text = (df[\"processed_text\"] != \"\") & (df[\"processed_text\"] != \" \") & (df[\"processed_text\"] != \"  \") \\\n",
    "    & (df[\"processed_text\"] != \"   \") \n",
    "df = df[filter_text]\n",
    "df.reset_index(inplace=True,drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-387b40aff60f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDIR_DATA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'clean_data_Event.tsv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.to_pickle(path.join(DIR_DATA, 'clean_data_Event.tsv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#filtering = df['year'] == 2016\n",
    "#df = df[filtering]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.reset_index(drop = True,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loading the saved file is as easy as running these lines of code\n",
    "with open(path.join(DIR_DATA, 'clean_data_Event.tsv'), 'rb') as in_file:\n",
    "    df = pickle.load(in_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# As there are so many tweets about the weather, we postprocess the data by removing the tweets with removing them\n",
    "Weather_words = ['falling', 'CForecast','humidity','pressure','wind','temperature ', 'Conditions', 'Cloudy', 'hpa', 'humidite', 'info',\\\n",
    "                 'km','Rain','fog','foggy','cloudy','kmh', 'mm', 'pluie', 'pression', 'temp','Temp', 'vent']\n",
    "filtering = df['processed_text'].apply(lambda x:not any(w in Weather_words for w in x.split()))\n",
    "df = df[filtering]\n",
    "df.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "stop_words.extend(nltk.corpus.stopwords.words('french'))\n",
    "stop_words.extend(nltk.corpus.stopwords.words('italian'))\n",
    "stop_words.extend(nltk.corpus.stopwords.words('german'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load nltk's SnowballStemmer as variabled 'stemmer'\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to our search in the literutre we found that stemming is not that helpful, in the case one want ot do stemming, first he should detect the language (for example by langdetect package) and then use the stemmer developed for that language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#not super pythonic, no, not at all.\n",
    "#use extend so it's a big flat list of vocab\n",
    "totalvocab_stemmed = []\n",
    "totalvocab_tokenized = []\n",
    "for index,row in df.iterrows():\n",
    "    tex = row['processed_text']\n",
    "    filtered_words = [str(word.lower()) for word in tex.split() if word.lower() not in stop_words]\n",
    "    filtered_words = ' '.join(filtered_words)\n",
    "    allwords_stemmed = tokenize_and_stem(filtered_words) #for each item in 'synopses', tokenize/stem\n",
    "    totalvocab_stemmed.extend(allwords_stemmed) #extend the 'totalvocab_stemmed' list\n",
    "    \n",
    "    allwords_tokenized = tokenize_only(filtered_words)\n",
    "    totalvocab_tokenized.extend(allwords_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "361753"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(totalvocab_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "361753"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(totalvocab_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 361753 items in vocab_frame\n"
     ]
    }
   ],
   "source": [
    "vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)\n",
    "print('there are ' + str(vocab_frame.shape[0]) + ' items in vocab_frame')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# finding the language of the texts\n",
    "def find_lang(text):\n",
    "    try:\n",
    "        result = guess_language(text)\n",
    "    except:\n",
    "        result = np.nan\n",
    "    return result\n",
    "#df2016_noW['language'] = df2016_noW['processed_text'].apply(find_lang)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "frequency = df.groupby('language').count()\n",
    "frequency.sort('id', ascending=False,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.84 s, sys: 15.5 ms, total: 1.85 s\n",
      "Wall time: 1.85 s\n",
      "(10000, 50)\n"
     ]
    }
   ],
   "source": [
    "# As the \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#define vectorizer parameters\n",
    "tfidf_vectorizer = TfidfVectorizer( max_features=200000,\n",
    "                                 min_df= 0.01, stop_words='english',\n",
    "                                 use_idf=True, tokenizer=tokenize_only, ngram_range=(1,3))\n",
    "\n",
    "%time tfidf_matrix = tfidf_vectorizer.fit_transform(df.iloc[:10000]['processed_text']) #fit the vectorizer to synopses\n",
    "\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "terms = tfidf_vectorizer.get_feature_names() # some possible features for being a stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "dist = 1 - cosine_similarity(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 605 ms, sys: 2.21 ms, total: 607 ms\n",
      "Wall time: 608 ms\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "num_clusters = 5\n",
    "\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "\n",
    "%time km.fit(tfidf_matrix)\n",
    "\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "#uncomment the below to save your model \n",
    "#since I've already run my model I am loading from the pickle\n",
    "\n",
    "joblib.dump(km,  'doc_cluster.pkl')\n",
    "\n",
    "#km = joblib.load('doc_cluster.pkl')\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "films = { 'title': df['processed_text'], 'cluster': clusters }\n",
    "\n",
    "frame = pd.DataFrame(films, index = [clusters] , columns = ['title','processed_text', 'cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    8397\n",
       "2     503\n",
       "1     470\n",
       "4     397\n",
       "3     233\n",
       "Name: cluster, dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame['cluster'].value_counts() #number of films per cluster (clusters from 0 to 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "\n",
      "Cluster 0 words: juste, t, switzerland, good, like, amp,\n",
      "\n",
      "Cluster 0 titles: The Rockefeller Foundation s Bellagio Centre A place to inspire creative thinking  ,\n",
      "\n",
      "Cluster 1 words: nan, worked, fandom, juste, help, help,\n",
      "\n",
      "Cluster 1 titles:  cannot find you on linkedin  but here we are,\n",
      "\n",
      "Cluster 2 words: s, juste, today, t, good, like,\n",
      "\n",
      "Cluster 2 titles: Great insight into the complex drivng factors of extremism  Deeyah Khan s film  Jihad a story of others   ,\n",
      "\n",
      "Cluster 3 words: sharing, sign, sign, sharing, sign, nan,\n",
      "\n",
      "Cluster 3 titles: Ratatatatatatatat,\n",
      "\n",
      "Cluster 4 words: m, s, geneva, amp, juste, t,\n",
      "\n",
      "Cluster 4 titles: In   hrs yesterday one of our local outdoor pools was transformed into a refuge camp for     people – expected soon ,\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "print(\"Top terms per cluster:\")\n",
    "print()\n",
    "#sort cluster centers by proximity to centroid\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
    "\n",
    "for i in range(num_clusters):\n",
    "    print(\"Cluster %d words:\" % i, end='')\n",
    "    \n",
    "    for ind in order_centroids[i, :6]: #replace 6 with n words per cluster\n",
    "        print(' %s' % vocab_frame.ix[terms[ind].split(' ')].values.tolist()[0][0], end=',')\n",
    "    print() #add whitespace\n",
    "    print() #add whitespace\n",
    "    \n",
    "    print(\"Cluster %d titles:\" % i, end='')\n",
    "    print(' %s,' % frame.loc[i]['title'].iloc[0], end='')\n",
    "    print() #add whitespace\n",
    "    print() #add whitespace\n",
    "    \n",
    "print()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Great insight into the complex drivng factors of extremism  Deeyah Khan s film  Jihad a story of others   '"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame.iloc[0]['title']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Rockefeller Foundation s Bellagio Centre A place to inspire creative thinking  '"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame.iloc[1]['title']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In   hrs yesterday one of our local outdoor pools was transformed into a refuge camp for     people – expected soon '"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame.loc[4]['title'].iloc[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(frame.loc[0]['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
