{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import path\n",
    "from time import sleep, time\n",
    "import csv\n",
    "from urllib.request import urlretrieve\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from geopy.geocoders import Nominatim\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon, MultiPolygon\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading sample.tsv file...\n",
      "is done!\n"
     ]
    }
   ],
   "source": [
    "# Read the csv file\n",
    "print('Reading sample.tsv file...')\n",
    "df = pd.read_csv(\n",
    "    path.join('data', 'sample.tsv'),\n",
    "    sep=\"\\t\",\n",
    "    encoding='utf-8',\n",
    "    escapechar='\\\\',\n",
    "    na_values='N',\n",
    "    quoting=csv.QUOTE_NONE,\n",
    "    header=None\n",
    ")\n",
    "print('is done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading schema.txt file...\n",
      "is done!\n"
     ]
    }
   ],
   "source": [
    "print('Reading schema.txt file...')\n",
    "schema = pd.read_csv(\n",
    "    path.join('data', 'schema.txt'),\n",
    "    sep=\"\\s+\",\n",
    "    header =None\n",
    ")\n",
    "print('is done!')\n",
    "# Rename the dataframe columns\n",
    "df.columns = schema[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Drop rows with NaN values in important columns\n",
    "df.dropna(\n",
    "    subset=['createdAt','placeLatitude','placeLongitude','userId','id'],\n",
    "    how='any',\n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Change the string in 'createdAt' column to datetime format\n",
    "df['createdAt'] = pd.to_datetime(\n",
    "    df['createdAt'],\n",
    "    format='%Y-%m-%d %H:%M:%S',\n",
    "    errors='coerce'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to recover the cities location from latitude-longitude pairs, we use two different strategies:\n",
    "\n",
    "1. **online strategy:** we use the geopy API to send a request containing information about the longitude and latitude of a place. The main cumbersome here is that all these kind of online APIs have some kind of request rate limit, and as it is suggested in [its website](http://wiki.openstreetmap.org/wiki/Nominatim_usage_policy), the time between two consecutive request should be more that 1 seconds. This actually makes the online approach so slow. One remedy to accelerate the process is to save longitude-latitude: location pair to a dictionary. Thus, before sending a request, we first check whether we have the location in our dictionary or not.\n",
    "\n",
    "2. **offline strategy:** we can also use the geojson or topojson files for Switzerland and its neighbor countries. The corresponding geofiles are downloaded from the following github repositories:\n",
    "    1. Switzerland topojson file from [swiss_map repo](https://github.com/d-qn/swiss-maps).\n",
    "    2. France geojson file from [france-geojson repo](https://github.com/gregoiredavid/france-geojson).\n",
    "    3. Italy geojson file from [leaflet-geojson-selector repo](https://github.com/stefanocudini/leaflet-geojson-selector).\n",
    "    4. Germany geojson file from [deutschlandGeoJSON repo](https://github.com/isellsoap/deutschlandGeoJSON)\n",
    "    5. Austria geojson file from [click_that_hood repo](https://github.com/codeforamerica/click_that_hood).\n",
    "\n",
    "In general, for the offinle strategy, one can also follow this [stackoverflow response](http://stackoverflow.com/questions/6159074/given-the-lat-long-coordinates-how-can-we-find-out-the-city-country/6355183#6355183) or this [one](http://stackoverflow.com/a/24871449/5267664). The first one relies on the geoname database while the second one actually gives us a procedure to find geojson files for any country."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Online strategy:\n",
    "We start from the online strategy. The following function find the state/canton of a location together with its county. We will see later that we could do the same thing in the offline approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function for finding a location from the latitude-longitude information using online API\n",
    "geolocator = Nominatim()\n",
    "locations = dict()\n",
    "settlements = {'city', 'town', 'village', 'hamlet', 'isolated_dwelling'}\n",
    "def online_locating(data):\n",
    "    lat = str(round(data.placeLatitude,2))\n",
    "    lng = str(round(data.placeLongitude,2))\n",
    "    lookup = ','.join([lat, lng])\n",
    "    if lookup not in set(locations.keys()):\n",
    "        location = geolocator.reverse(lookup, language='en')\n",
    "        try:\n",
    "            state = location.raw['address']['state']\n",
    "        except:\n",
    "            state = float('NaN')\n",
    "        sets_intersect = settlements.intersection(set(location.raw['address'].keys()))\n",
    "        try:\n",
    "            settlement = location.raw['address']['county']\n",
    "        except:\n",
    "            try:\n",
    "                settlement = location.raw['address'][list(sets_intersect)[0]]\n",
    "            except:\n",
    "                settlement = float('NaN')\n",
    "        locations[lookup] = {'state': state, 'settlement': settlement}\n",
    "        sleep(1) # sleep for 1 sec (required by Nominatim usage policy)\n",
    "    return pd.Series({'state': locations[lookup]['state'],\n",
    "                      'settlement': locations[lookup]['settlement']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t = time()\n",
    "df[['state', 'settlement']] = df.apply(lambda x: online_locating(x), axis=1)\n",
    "elapsed = time() - t\n",
    "print('Elapsed time is ' + str(round(elapsed, 4)) + ' seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, it took very long to even recover locations for the sample file. Hence, it does not make sense to follow the online approach for the actual problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Offline approach:\n",
    "As we mentioned before, it is necessary to download the required geojson/topojson file to run the offline approach. All files are available in data/geofiles folder. In this part, we use [gepandas](http://geopandas.org/) for furthur analysis. The resulting dataframes can be used to find the location of tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ch_gdf = gpd.read_file(path.join('data/geofiles', 'ch-cantons.json'))\n",
    "fr_gdf = gpd.read_file(path.join('data/geofiles', 'france-states.geojson'))\n",
    "it_gdf = gpd.read_file(path.join('data/geofiles', 'italy-states.json'))\n",
    "de_gdf = gpd.read_file(path.join('data/geofiles', 'germany-states.geojson'))\n",
    "at_gdf = gpd.read_file(path.join('data/geofiles', 'austria-states.geojson'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Modify dataframes for merging\n",
    "ch_gdf = ch_gdf[['geometry', 'name']]\n",
    "ch_gdf['country'] = 'CH'\n",
    "fr_gdf = fr_gdf[['geometry', 'name']]\n",
    "fr_gdf['country'] = 'FR'\n",
    "it_gdf = it_gdf[['geometry', 'name']]\n",
    "it_gdf['country'] = 'IT'\n",
    "de_gdf = de_gdf[['geometry', 'NAME_1']]\n",
    "de_gdf = de_gdf.rename(columns={'NAME_1': 'name'})\n",
    "de_gdf['country'] = 'DE'\n",
    "at_gdf = at_gdf[['geometry', 'name']]\n",
    "at_gdf['country'] = 'AT'\n",
    "gdf = pd.concat([ch_gdf, fr_gdf, it_gdf, de_gdf, at_gdf], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The R-tree structure in geopandas dataframe enables us to find the twitters location very fast. The following function find each location is inside which state/canton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "neighbors_df = (\n",
    "    pd.read_csv(\n",
    "        path.join('data/geonames', fname + '.txt'),\n",
    "        header=None,\n",
    "        encoding='utf8',\n",
    "        delimiter='\\t',\n",
    "        dtype={9: str},\n",
    "        names=col_names,\n",
    "        low_memory=False\n",
    "    )\n",
    "    for fname in ['DE', 'FR', 'IT', 'AT']\n",
    ")\n",
    "neighbors_df = pd.concat(neighbors_df, ignore_index=True)\n",
    "neighbors_df = neighbors_df[neighbors_df['feature class'].str.contains('P')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also reduce our search space size for neighbors countries by considering the states sharing border line with Switzerland."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imp_states = [\n",
    "    'FR.84', 'FR.27', 'FR.44', # Auvergne-Rhône-Alpes, Bourgogne-Franche-Comte, and Grand Est \n",
    "    'DE.01', 'DE.02', # Bavaria and Baden-Wuerttemberg\n",
    "    'AT.07', 'AT.08', # Tyrol and Vorarlberg\n",
    "    'IT.19', 'IT.09', 'IT.12', 'IT.17' # Aosta Valley, Lombardy, Piedmont, and Trentino-Alto Adige\n",
    "]\n",
    "neighbors_df['country code'] + .head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful links:\n",
    "http://stackoverflow.com/questions/17267248/how-where-do-i-get-geojson-data-for-states-provinces-and-administrative-region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = {1, 2, 3}\n",
    "b = {3, 4}\n",
    "c = a.intersection(b)\n",
    "if c:\n",
    "    print('Ture')\n",
    "else:\n",
    "    print('False')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import osmnx as ox\n",
    "import geopandas as gpd\n",
    "from descartes import PolygonPatch\n",
    "from shapely.geometry import Point, Polygon, MultiPolygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gdf = ox.graph_from_place('brig', network_type='drive')\n",
    "ox.plot_graph(ox.project_graph(gdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "city = ox.gdf_from_place('brig')\n",
    "ox.save_gdf_shapefile(city)\n",
    "city = ox.project_gdf(city)\n",
    "fig, ax = ox.plot_shape(city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get the boundary of some city\n",
    "gdf = ox.gdf_from_place('Switzerland')\n",
    "\n",
    "# get the street network within this bounding box\n",
    "west, south, east, north = gdf.unary_union.buffer(0.1).bounds\n",
    "G = ox.graph_from_bbox(north, south, east, west, network_type='drive', retain_all=True)\n",
    "\n",
    "# get lat-long points for each intersection\n",
    "xy = [(data['x'], data['y']) for node, data in G.nodes(data=True)]\n",
    "x, y = list(zip(*xy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# turn the lat-long points into a geodataframe\n",
    "gdf_nodes = gpd.GeoDataFrame(data={'x':x, 'y':y})\n",
    "gdf_nodes.crs = gdf.crs\n",
    "gdf_nodes.name = 'nodes'\n",
    "gdf_nodes['geometry'] = gdf_nodes.apply(lambda row: Point((row['x'], row['y'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "geometry = gdf['geometry'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if isinstance(geometry, Polygon):\n",
    "    geometry = MultiPolygon([geometry])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "west, south, east, north = gdf.unary_union.bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "west, south, east, north = gdf.unary_union.bounds\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "for polygon in geometry:\n",
    "    print('yes')\n",
    "    patch = PolygonPatch(polygon, fc='#cccccc', ec='k', alpha=0.5, zorder=2)\n",
    "    ax.add_patch(patch)\n",
    "    \n",
    "ax.set_xlim(west, east)\n",
    "ax.set_ylim(south, north)\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_gpd = gpd.read_file(path.join('data', 'ch.json'), layer='municipalities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = float('NaN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
