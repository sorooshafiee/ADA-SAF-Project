{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pickle\n",
    "from os import path\n",
    "from time import time, sleep\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from geopy.distance import vincenty\n",
    "from geopy.geocoders import Nominatim\n",
    "from difflib import get_close_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Settings\n",
    "DIR_DATA = path.join('..', 'data', 'sample data')\n",
    "DIR_GEO = path.join('..', 'data', 'geofiles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading the saved file\n",
    "with open(path.join(DIR_DATA, 'clean_data.pkl'), 'rb') as in_file:\n",
    "    df = pickle.load(in_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_denoising(sub_df, crt_speed=60):\n",
    "    \"\"\" This function aims to identify noisy tweets. By the term noisy, we mean that the\n",
    "    reported location for the tweet is noisy.\"\"\"\n",
    "    zipped_columns = sub_df.tolist()\n",
    "    lst = list(zip(*zipped_columns))\n",
    "    lat = lst[0]\n",
    "    lng = lst[1]\n",
    "    tw_time = lst[2]\n",
    "    denoised = [True] * len(sub_df)\n",
    "    points = list(zip(lat, lng))\n",
    "\n",
    "    orig = points[0:-2]\n",
    "    dest1 = points[1:-1]\n",
    "    dest2 = points[2::]\n",
    "\n",
    "    for index in range(len(orig)):\n",
    "        d1 = vincenty(dest1[index], orig[index]).meters\n",
    "        t1 = tw_time[index+1] - tw_time[index]\n",
    "        t1 = t1.total_seconds()\n",
    "        v1 = d1 / t1 if t1 else float('inf')\n",
    "\n",
    "        d2 = vincenty(dest2[index], dest1[index]).meters\n",
    "        t2 = tw_time[index+2] - tw_time[index+1]\n",
    "        t2 = t2.total_seconds()\n",
    "        v2 = d2 / t2 if t2 else float('inf')\n",
    "\n",
    "        d3 = vincenty(dest2[index], orig[index]).meters\n",
    "        t3 = tw_time[index+2] - tw_time[index]\n",
    "        t3 = t3.total_seconds()\n",
    "        v3 = d3 / t3 if t3 else float('inf')\n",
    "\n",
    "        if np.isinf(v1) | np.isinf(v2) | np.isinf(v3):\n",
    "            denoised = [False] * len(denoised)\n",
    "            break\n",
    "        if (v1 > crt_speed) & (v2 > crt_speed):\n",
    "            if v3 <= crt_speed:\n",
    "                denoised[index+1] = False\n",
    "            else:\n",
    "                denoised[index] = False\n",
    "                denoised[index+1] = False\n",
    "                if index == len(orig) - 1:\n",
    "                    denoised[index+2] = False\n",
    "        if (v1 > crt_speed) & (v2 <= crt_speed):\n",
    "            denoised[index] = False\n",
    "\n",
    "    return denoised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove noisy tweets with the above function\n",
    "daily_user = ['userId', 'year', 'month', 'day']\n",
    "df['new'] = tuple(zip(df['latitude'], df['longitude'], df['createdAt']))\n",
    "not_noisy = df.groupby(by=daily_user)['new'].transform(lambda x: data_denoising(x))\n",
    "df = df[not_noisy].reset_index(drop=True)\n",
    "# Remove the generated column\n",
    "del df['new']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to recover the cities location from latitude-longitude pairs, we use two different strategies:\n",
    "\n",
    "1. **online strategy:** we use the geopy API to send a request containing information about the longitude and latitude of a place. The main cumbersome here is that all these kind of online APIs have some kind of request rate limit, and as it is suggested in [its website](http://wiki.openstreetmap.org/wiki/Nominatim_usage_policy), the time between two consecutive request should be more that 1 seconds. This actually makes the online approach so slow. One remedy to accelerate the process is to save longitude-latitude: location pair to a dictionary. Thus, before sending a request, we first check whether we have the location in our dictionary or not.\n",
    "\n",
    "2. **offline strategy:** we can also use the geojson or topojson files for Switzerland and its neighbor countries. The corresponding geofiles are downloaded from the following github repositories:\n",
    "    1. Switzerland topojson file from [swiss_map repo](https://github.com/interactivethings/swiss-maps).\n",
    "    2. France geojson file from [france-geojson repo](https://github.com/gregoiredavid/france-geojson).\n",
    "    3. Italy geojson file from [leaflet-geojson-selector repo](https://github.com/stefanocudini/leaflet-geojson-selector).\n",
    "    4. Germany geojson file from [deutschlandGeoJSON repo](https://github.com/isellsoap/deutschlandGeoJSON)\n",
    "    5. Austria geojson file from [click_that_hood repo](https://github.com/codeforamerica/click_that_hood).\n",
    "    6. Liechtenstein geojson file from [CountryGeoJSONCollection repo](https://github.com/LonnyGomes/CountryGeoJSONCollection).\n",
    "\n",
    "In general, for the offinle strategy, one can also follow this [stackoverflow response](http://stackoverflow.com/questions/6159074/given-the-lat-long-coordinates-how-can-we-find-out-the-city-country/6355183#6355183) or this [one](http://stackoverflow.com/a/24871449/5267664). The first one relies on the geoname database while the second one actually gives us a procedure to find geojson files for any country."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Online strategy:\n",
    "We start from the online strategy. The following function find the country together with its state/canton of a location. We will see later that we could do the same thing in the offline approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function for finding a location from the latitude-longitude information using online API\n",
    "geolocator = Nominatim()\n",
    "locations = dict()\n",
    "def online_locating(data):\n",
    "    lat = str(data.latitude)\n",
    "    lng = str(data.longitude)\n",
    "    lookup = ','.join([lat, lng])\n",
    "    if lookup not in set(locations.keys()):\n",
    "        try:\n",
    "            location = geolocator.reverse(lookup, language='en')\n",
    "        except TimeOut:\n",
    "            online_locating(data)                \n",
    "        try:\n",
    "            country = location.raw['address']['country_code'].upper()\n",
    "        except:\n",
    "            country = float('NaN')\n",
    "        try:\n",
    "            state = location.raw['address']['state']\n",
    "        except:\n",
    "            try:\n",
    "                state = location.raw['address']['country']\n",
    "            except:\n",
    "                state = float('NaN')\n",
    "        locations[lookup] = {'country': country, 'state': state}\n",
    "        sleep(1) # sleep for 1 sec (required by Nominatim usage policy)\n",
    "    return pd.Series({'country': locations[lookup]['country'],\n",
    "                      'state': locations[lookup]['state']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time is 312.38 seconds.\n"
     ]
    }
   ],
   "source": [
    "online_df = df.copy()\n",
    "t = time()\n",
    "online_df[['country', 'state']] = online_df.apply(lambda x: online_locating(x), axis=1)\n",
    "elapsed = time() - t\n",
    "print('Elapsed time is ' + str(round(elapsed, 2)) + ' seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, it took very long to even recover locations for the sample file. Hence, it does not make sense to follow the online approach for the actual problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Offline approach:\n",
    "As we mentioned before, it is necessary to download the required geojson/topojson file to run the offline approach. All files are available in data/geofiles folder. In this part, we use [gepandas](http://geopandas.org/) for furthur analysis. The resulting dataframes can be used to find the location of tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ch_gdf = gpd.read_file(path.join('..', 'data', 'geofiles', 'ch-cantons.json'))\n",
    "fr_gdf = gpd.read_file(path.join('..', 'data', 'geofiles', 'france-states.geojson'))\n",
    "it_gdf = gpd.read_file(path.join('..', 'data', 'geofiles', 'italy-states.json'))\n",
    "de_gdf = gpd.read_file(path.join('..', 'data', 'geofiles', 'germany-states.geojson'))\n",
    "at_gdf = gpd.read_file(path.join('..', 'data', 'geofiles', 'austria-states.geojson'))\n",
    "li_gdf = gpd.read_file(path.join('..', 'data', 'geofiles', 'liechtenstein.geojson'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Modify dataframes for merging\n",
    "ch_gdf = ch_gdf[['geometry', 'name']]\n",
    "ch_gdf['country'] = 'CH'\n",
    "\n",
    "fr_gdf = fr_gdf[['geometry', 'name']]\n",
    "fr_gdf['country'] = 'FR'\n",
    "\n",
    "it_gdf = it_gdf[['geometry', 'name']]\n",
    "it_gdf['country'] = 'IT'\n",
    "\n",
    "de_gdf = de_gdf[['geometry', 'NAME_1']]\n",
    "de_gdf = de_gdf.rename(columns={'NAME_1': 'name'})\n",
    "de_gdf['country'] = 'DE'\n",
    "\n",
    "at_gdf = at_gdf[['geometry', 'name']]\n",
    "at_gdf['country'] = 'AT'\n",
    "\n",
    "li_gdf = li_gdf[['geometry', 'NAME']]\n",
    "li_gdf = li_gdf.rename(columns={'NAME': 'name'})\n",
    "li_gdf['country'] = 'LI'\n",
    "\n",
    "df_poly = pd.concat([ch_gdf, fr_gdf, it_gdf, de_gdf, at_gdf, li_gdf], ignore_index=True)\n",
    "df_poly = df_poly.rename(columns={'name': 'state'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The R-tree structure in geopandas dataframe enables us to find the twitters location very fast. The following function find each location is inside which state/canton. A tutorial to show how to take advantages of R-tree structure is available [here](http://geoffboeing.com/2016/10/r-tree-spatial-index-python/#more-2183)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = gpd.GeoDataFrame(df)\n",
    "df['geometry'] = df.apply(lambda row: Point(row.longitude, row.latitude), axis=1)\n",
    "df.crs = df_poly.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time is 0.7876 seconds.\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "offline_gdf = gpd.tools.sjoin(df, df_poly, how=\"left\")\n",
    "elapsed = time() - t\n",
    "print('Elapsed time is ' + str(round(elapsed, 4)) + ' seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "offline_gdf.drop_duplicates(subset='id', inplace=True)\n",
    "offline_gdf.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there are still some tweets which we are not able to find its location. For these tweets, we call the online_locating function in order to find the location of the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "null_index = offline_gdf['state'].isnull()\n",
    "offline_gdf.loc[null_index,['country', 'state']] = offline_gdf[null_index].apply(\n",
    "    lambda row: online_locating(row), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There also some inconsistansy between the geolacating by offline approach and online approach. The following function aims to remove these inconsistansies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def modify_dataframe(row):\n",
    "    countries = set(df_poly['country'].unique())\n",
    "    if row['country'] not in countries:\n",
    "        row['state'] = row['country']\n",
    "    else:\n",
    "        sub_gdf = df_poly[df_poly['country'] == row['country']]\n",
    "        states = sub_gdf.state.values\n",
    "        if 'Bavaria' in row['state']:\n",
    "            row['state'] = 'Bayern'\n",
    "        elif row['state'] == 'Great East':\n",
    "            row['state'] = 'Alsace-Champagne-Ardenne-Lorraine'\n",
    "        elif row['state'] == 'Grisons':\n",
    "            row['state'] = 'Graubünden'\n",
    "        elif row['state'] == 'Aosta Valley':\n",
    "            row['state'] = \"valle d'aosta\"\n",
    "        else:\n",
    "            row['state'] = get_close_matches(row['state'], states, 1, 0)[0]\n",
    "        row['index_right'] = sub_gdf[sub_gdf['state'] == row['state']].index.values[0]\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "offline_gdf.loc[null_index,:] = offline_gdf[null_index].apply(\n",
    "    lambda row: modify_dataframe(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The error in locating country is %0.0\n",
      "The error in locating state is %0.46\n"
     ]
    }
   ],
   "source": [
    "online_df = online_df.apply(lambda row: modify_dataframe(row), axis=1)\n",
    "print('The error in locating country is %' + str(\n",
    "        round(100 * sum(online_df['country'] != offline_gdf['country'])/online_df.shape[0],2)))\n",
    "print('The error in locating state is %' + str(\n",
    "        round(100 * sum(online_df['state'] != offline_gdf['state'])/online_df.shape[0],2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove tweets from countries which are not in our list\n",
    "offline_gdf = offline_gdf[offline_gdf['index_right'].notnull()]\n",
    "\n",
    "# Reset index\n",
    "offline_gdf.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Remove index_right column\n",
    "del offline_gdf['index_right']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_path(sub_df, cmt_num = 10):       \n",
    "    origin = sub_df.values[0:-1]\n",
    "    destination = sub_df.values[1::]\n",
    "    index = origin != destination\n",
    "    path = float('NaN')\n",
    "    if (index.any()) & (sum(index) <= cmt_num):\n",
    "        path = '->'.join(origin[index])\n",
    "        path = '->'.join([path, destination[index][-1]])\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "daily_patterns = offline_gdf.groupby(by=daily_user)['state'].apply(lambda x: find_path(x))\n",
    "daily_patterns = daily_patterns.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the results with pickle\n",
    "with open(path.join(DIR_DATA, 'daily_patterns.pkl'), 'wb') as in_file:\n",
    "    pickle.dump(daily_patterns, in_file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
