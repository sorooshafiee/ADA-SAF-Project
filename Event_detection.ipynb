{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to a study by Pear Analytics [16], about 40% of all the tweets are pointless “babbles” like “have to get something from the minimart downstairs”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "import pickle\n",
    "from os import path\n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import nltk\n",
    "import csv\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import CMUTweetTagger\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "import scipy.cluster.hierarchy as sch\n",
    "import fastcluster\n",
    "from collections import Counter\n",
    "import codecs\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DIR_DATA = path.join('data', 'twitter data')\n",
    "DIR_GEO = path.join('data', 'geofiles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loading the saved file is as easy as running these lines of code\n",
    "#with open(path.join(DIR_DATA, 'clean_data.pkl'), 'rb') as in_file:\n",
    "#    df = pickle.load(in_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twex.tsv file...\n",
      "is done!\n"
     ]
    }
   ],
   "source": [
    "# Read the splitted tsv files\n",
    "all_files = glob(path.join(DIR_DATA, '*.tsv'))\n",
    "if path.join(DIR_DATA, 'twex.tsv') in all_files:\n",
    "    all_files.remove(path.join(DIR_DATA, 'twex.tsv'))\n",
    "\n",
    "df_from_each_file = (pd.read_csv(\n",
    "    file_name,\n",
    "    sep=\"\\t\",\n",
    "    encoding='utf-8',\n",
    "    escapechar='\\\\',\n",
    "    na_values='N',\n",
    "    quoting=csv.QUOTE_NONE,\n",
    "    header=None\n",
    "    )\n",
    "    for file_name in all_files)\n",
    "print('Reading twex.tsv file...')\n",
    "df = pd.concat(df_from_each_file, ignore_index=True)\n",
    "print('is done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.sort_values(by='createdAt', ascending=1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Here we normalize the text, the code is taken from \n",
    "#https://github.com/heerme/twitter-topics/blob/master/twitter-topics-from-json-text-stream.py\n",
    "def normalize_text(text):\n",
    "    if type(text) is not str:\n",
    "        print(text)\n",
    "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+)|(pic\\.twitter\\.com/[^\\s]+))','', text)\n",
    "    text = re.sub('@[^\\s]+','', text)\n",
    "    text = re.sub('#([^\\s]+)', '', text)\n",
    "    text = re.sub('[:;>?<=*+()/,\\-#!$%\\{˜|\\}\\[^_\\\\@\\]1234567890’‘]',' ', text)\n",
    "    text = re.sub('[\\d]','', text)\n",
    "    text = text.replace(\".\", '')\n",
    "    text = text.replace(\"'\", ' ')\n",
    "    text = text.replace(\"\\\"\", ' ')\n",
    "    #text = text.replace(\"-\", \" \")\n",
    "    #normalize some utf8 encoding\n",
    "    text = text.replace(\"\\x9d\",' ').replace(\"\\x8c\",' ')\n",
    "    text = text.replace(\"\\xa0\",' ')\n",
    "    text = text.replace(\"\\x9d\\x92\", ' ').replace(\"\\x9a\\xaa\\xf0\\x9f\\x94\\xb5\", ' ').replace(\"\\xf0\\x9f\\x91\\x8d\\x87\\xba\\xf0\\x9f\\x87\\xb8\", ' ').replace(\"\\x9f\",' ').replace(\"\\x91\\x8d\",' ')\n",
    "    text = text.replace(\"\\xf0\\x9f\\x87\\xba\\xf0\\x9f\\x87\\xb8\",' ').replace(\"\\xf0\",' ').replace('\\xf0x9f','').replace(\"\\x9f\\x91\\x8d\",' ').replace(\"\\x87\\xba\\x87\\xb8\",' ')\t\n",
    "    text = text.replace(\"\\xe2\\x80\\x94\",' ').replace(\"\\x9d\\xa4\",' ').replace(\"\\x96\\x91\",' ').replace(\"\\xe1\\x91\\xac\\xc9\\x8c\\xce\\x90\\xc8\\xbb\\xef\\xbb\\x89\\xd4\\xbc\\xef\\xbb\\x89\\xc5\\xa0\\xc5\\xa0\\xc2\\xb8\",' ')\n",
    "    text = text.replace(\"\\xe2\\x80\\x99s\", \" \").replace(\"\\xe2\\x80\\x98\", ' ').replace(\"\\xe2\\x80\\x99\", ' ').replace(\"\\xe2\\x80\\x9c\", \" \").replace(\"\\xe2\\x80\\x9d\", \" \")\n",
    "    text = text.replace(\"\\xe2\\x82\\xac\", \" \").replace(\"\\xc2\\xa3\", \" \").replace(\"\\xc2\\xa0\", \" \").replace(\"\\xc2\\xab\", \" \").replace(\"\\xf0\\x9f\\x94\\xb4\", \" \").replace(\"\\xf0\\x9f\\x87\\xba\\xf0\\x9f\\x87\\xb8\\xf0\\x9f\", \"\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find the hashtags and users\n",
    "df['Hashtags'] = df['text'].apply(lambda x:{tag.strip(\"#\") for tag in x.split() if tag.startswith(\"#\")})\n",
    "df['users'] = df['text'].apply(lambda x:{tag.strip(\"@\") for tag in x.split() if tag.startswith(\"@\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.dropna(subset = ['text'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['processed_text'] = df['text'].apply(lambda x: normalize_text(x))\n",
    "df.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df.iloc[:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  filter the blank cells\n",
    "filter_text = (df[\"processed_text\"] != \"\") & (df[\"processed_text\"] != \" \") & (df[\"processed_text\"] != \"  \") \\\n",
    "    & (df[\"processed_text\"] != \"   \") \n",
    "df = df[filter_text]\n",
    "df.reset_index(inplace=True,drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "stop_words.extend(nltk.corpus.stopwords.words('french'))\n",
    "stop_words.extend(nltk.corpus.stopwords.words('italian'))\n",
    "stop_words.extend(nltk.corpus.stopwords.words('german'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tknzr = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def nltk_tokenize(text):\n",
    "    tokens = []\n",
    "    pos_tokens = []\n",
    "    entities = []\n",
    "    features = []\n",
    "    try:\n",
    "        tokens = tknzr.tokenize(text)\n",
    "        #tokens = text.split()\n",
    "        for word in tokens:\n",
    "            if word.lower() not in stop_words and len(word) > 1:\n",
    "                features.append(word)\n",
    "    except: \n",
    "        pass\n",
    "    return [tokens, pos_tokens, entities, features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def custom_tokenize_text(text):\n",
    "    REGEX = re.compile(r\",\\s*\")\n",
    "    tokens = []\n",
    "    for tok in REGEX.split(text):\n",
    "        #if \"@\" not in tok and \"#\" not in tok:\n",
    "        if \"@\" not in tok:\n",
    "            tokens.append(tok.strip().lower())\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet_old_time = -1\n",
    "window_analysis_time = 20  # This is the size of window that we analyze text inside it\n",
    "\n",
    "tid_to_raw_tweet = {}\n",
    "window_corpus = []\n",
    "tid_to_urls_window_corpus = {}\n",
    "tids_window_corpus = []\n",
    "dfVocTimeWindows = {}\n",
    "t = 0\n",
    "ntweets = 0\n",
    "fout = codecs.open('Events', 'w', 'utf-8')\n",
    "for index, row in df.iterrows():\n",
    "    tweet_current_time = row['createdAt']\n",
    "    text = row['processed_text']\n",
    "    users = row['users']\n",
    "    hashtags = row['Hashtags']\n",
    "    if tweet_old_time == -1:\n",
    "        tweet_old_time = tweet_current_time\n",
    "    if (tweet_current_time - tweet_old_time).days < window_analysis_time: # Inside the window we still gather the data\n",
    "                                                                          # For analysis\n",
    "        ntweets += 1\n",
    "        [tokens, pos_tokens, entities, features] = nltk_tokenize(text)\n",
    "        tweet_bag = \"\"\n",
    "        try:\n",
    "            for user in set(users):\n",
    "                tweet_bag += \"@\" + user.decode('utf-8').lower() + \",\"\n",
    "            for tag in set(hashtags):\n",
    "                if tag.decode('utf-8').lower() not in stop_words: \n",
    "                    tweet_bag += \"#\" + tag.decode('utf-8').lower() + \",\"\n",
    "            for feature in features:\n",
    "                tweet_bag += feature + \",\"\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        if len(users) < 3 and len(hashtags) < 3 and len(features) > 3 and len(tweet_bag.split(\",\")) > 4 \\\n",
    "                        and not str(features).upper() == str(features):\n",
    "\n",
    "            tweet_bag = tweet_bag[:-1]\n",
    "            window_corpus.append(tweet_bag)\n",
    "            tids_window_corpus.append(row.id)\n",
    "            tid_to_raw_tweet[row.id] = text\n",
    "    else:\n",
    "        tweet_old_time = tweet_current_time\n",
    "        #increase window counter used in df-idf\n",
    "        t += 1\n",
    "        # The reason for min_df is that the cluster need to gather enough tweet to be considered a topic\n",
    "        # CountVectorizer: Convert a collection of text documents to a matrix of token counts\n",
    "        vectorizer = CountVectorizer(tokenizer=custom_tokenize_text, binary=True,\\\n",
    "                            min_df=max(int(len(window_corpus)*0.0025), 10), ngram_range=(2,3))\n",
    "        try: # If the number of tweet is not enough\n",
    "             # vectorizer.fit_transform Learn the vocabulary dictionary and return term-document matrix.\n",
    "            X = vectorizer.fit_transform(window_corpus)\n",
    "        except:\n",
    "            continue\n",
    "        map_index_after_cleaning = {}\n",
    "        Xclean = np.zeros((1, X.shape[1]))\n",
    "        for i in range(0, X.shape[0]):\n",
    "            #keep sample with size at least 5\n",
    "            if X[i].sum() > 4:\n",
    "                Xclean = np.vstack([Xclean, X[i].toarray()])\n",
    "                map_index_after_cleaning[Xclean.shape[0] - 2] = i\n",
    "        Xclean = Xclean[1:,]\n",
    "        #print(\"total tweets in window:\", ntweets)\n",
    "        #print(\"X.shape:\", X.shape)\n",
    "        #print(\"Xclean.shape:\", Xclean.shape)\n",
    "        X = Xclean\n",
    "        Xdense = np.matrix(X).astype('float')\n",
    "        # doing some preprocessing to make the \n",
    "        #data suitable for machin learning algorithms\n",
    "        X_scaled = preprocessing.scale(Xdense)\n",
    "        X_normalized = preprocessing.normalize(X_scaled, norm='l2')\n",
    "        vocX = vectorizer.get_feature_names() # Array mapping from feature integer indices to feature name\n",
    "        boost_entity = {}\n",
    "        pos_tokens = CMUTweetTagger.runtagger_parse([term.upper() for term in vocX],\\\n",
    "                                           run_tagger_cmd=\"java -XX:ParallelGCThreads=2 -Xmx500m -jar data/ark-tweet-nlp-0.3.2.jar\")\n",
    "        \n",
    "\n",
    "        for l in pos_tokens:\n",
    "            term =''\n",
    "            for gr in range(0, len(l)):\n",
    "                term += l[gr][0].lower() + \" \"\n",
    "            if \"^\" in str(l):\n",
    "                boost_entity[term.strip()] = 2.5\n",
    "            else: \n",
    "                boost_entity[term.strip()] = 1.0\n",
    "        dfX = X.sum(axis=0)\n",
    "        dfVoc = {}\n",
    "        wdfVoc = {}\n",
    "        boosted_wdfVoc = {}\n",
    "        keys = vocX\n",
    "        vals = dfX\n",
    "        for k,v in zip(keys, vals):\n",
    "            dfVoc[k] = v\n",
    "        for k in dfVoc: \n",
    "            try:\n",
    "                dfVocTimeWindows[k] += dfVoc[k]\n",
    "                avgdfVoc = (dfVocTimeWindows[k] - dfVoc[k])/(t - 1)\n",
    "            except:\n",
    "                dfVocTimeWindows[k] = dfVoc[k]\n",
    "                avgdfVoc = 0\n",
    "            wdfVoc[k] = (dfVoc[k] + 1) / (np.log(avgdfVoc + 1) + 1)\n",
    "            try:\n",
    "                boosted_wdfVoc[k] = wdfVoc[k] * boost_entity[k]\n",
    "            except: \n",
    "                boosted_wdfVoc[k] = wdfVoc[k]\n",
    "        #print(\"sorted wdfVoc*boost_entity:\")\n",
    "        #print(sorted( ((v,k) for k,v in boosted_wdfVoc.items()), reverse=True))\n",
    "        distMatrix = pairwise_distances(X_normalized, metric='cosine')\n",
    "        L = fastcluster.linkage(distMatrix, method='average')\n",
    "        dt = 1  # distance threshold for clustering\n",
    "        indL = sch.fcluster(L, dt*distMatrix.max(), 'distance')\n",
    "        freqTwCl = Counter(indL)\n",
    "        #print(\"n_clusters:\", len(freqTwCl))\n",
    "        #print(freqTwCl)\n",
    "        npindL = np.array(indL)\n",
    "        freq_th = max(10, int(X.shape[0]*0.0025))\n",
    "        cluster_score = {}\n",
    "        for clfreq in freqTwCl.most_common(50):\n",
    "            cl = clfreq[0]\n",
    "            freq = clfreq[1]\n",
    "            cluster_score[cl] = 0\n",
    "            if freq >= freq_th:\n",
    "                clidx = (npindL == cl).nonzero()[0].tolist()\n",
    "                cluster_centroid = X[clidx].sum(axis=0)\n",
    "                try:\n",
    "                    cluster_tweet = vectorizer.inverse_transform(cluster_centroid)\n",
    "                    for term in np.nditer(cluster_tweet):\n",
    "                        try:\n",
    "                            cluster_score[cl] = max(cluster_score[cl], boosted_wdfVoc[str(term).strip()])\n",
    "                        except: pass\n",
    "                except: pass\n",
    "                cluster_score[cl] /= freq\n",
    "            else: break\n",
    "        sorted_clusters = sorted( ((v,k) for k,v in cluster_score.items()), reverse=True)\n",
    "        #print(\"sorted cluster\",sorted_clusters)\n",
    "        ntopics = 20\n",
    "        headline_corpus = []\n",
    "        orig_headline_corpus = []\n",
    "        headline_to_cluster = {}\n",
    "        headline_to_tid = {}\n",
    "        cluster_to_tids = {}\n",
    "        for score,cl in sorted_clusters[:ntopics]:\n",
    "            clidx = (npindL == cl).nonzero()[0].tolist()\n",
    "            first_idx = map_index_after_cleaning[clidx[0]]\n",
    "            keywords = window_corpus[first_idx]\n",
    "            orig_headline_corpus.append(keywords)\n",
    "            headline = ''\n",
    "            for k in keywords.split(\",\"):\n",
    "                if not '@' in k and not '#' in k:\n",
    "                    headline += k + \",\"\n",
    "            headline_corpus.append(headline[:-1])\n",
    "            headline_to_cluster[headline[:-1]] = cl\n",
    "            headline_to_tid[headline[:-1]] = tids_window_corpus[first_idx]\n",
    "\n",
    "            tids = []\n",
    "            for i in clidx:\n",
    "                idx = map_index_after_cleaning[i]\n",
    "                tids.append(tids_window_corpus[idx])\n",
    "            cluster_to_tids[cl] = tids\n",
    "        ## cluster headlines to avoid topic repetition\n",
    "        headline_vectorizer = CountVectorizer(tokenizer=custom_tokenize_text, binary=True, min_df=1, ngram_range=(1,1))\n",
    "        H = headline_vectorizer.fit_transform(headline_corpus)\n",
    "        vocH = headline_vectorizer.get_feature_names()\n",
    "        Hdense = np.matrix(H.todense()).astype('float')\n",
    "        distH = pairwise_distances(Hdense, metric='cosine')\n",
    "        HL = fastcluster.linkage(distH, method='average')\n",
    "        dtH = 1.0\n",
    "        try:\n",
    "            indHL = sch.fcluster(HL, dtH*distH.max(), 'distance')\n",
    "        except:\n",
    "            continue\n",
    "        freqHCl = Counter(indHL)\n",
    "        npindHL = np.array(indHL)\n",
    "        hcluster_score = {}\n",
    "        for hclfreq in freqHCl.most_common(ntopics):\n",
    "            hcl = hclfreq[0]\n",
    "            hfreq = hclfreq[1]\n",
    "            hcluster_score[hcl] = 0\n",
    "            hclidx = (npindHL == hcl).nonzero()[0].tolist()\n",
    "            for i in hclidx:\n",
    "                hcluster_score[hcl] = max(hcluster_score[hcl], cluster_score[headline_to_cluster[headline_corpus[i]]])\n",
    "        sorted_hclusters = sorted( ((v,k) for k,v in hcluster_score.items()), reverse=True)\n",
    "        print(\"sorted hcluster_score:\",sorted_hclusters)\n",
    "        for hscore, hcl in sorted_hclusters[:10]:\n",
    "            hclidx = (npindHL == hcl).nonzero()[0].tolist()\n",
    "            clean_headline = ''\n",
    "            raw_headline = ''\n",
    "            keywords = ''\n",
    "            tids_set = set()\n",
    "            tids_list = []\n",
    "            urls_list = []\n",
    "            selected_raw_tweets_set = set()\n",
    "            tids_cluster = []\n",
    "            for i in hclidx:\n",
    "                clean_headline += headline_corpus[i].replace(\",\", \" \") + \"//\"\n",
    "                keywords += orig_headline_corpus[i].lower() + \",\"\n",
    "                tid = headline_to_tid[headline_corpus[i]]\n",
    "                tids_set.add(tid)\n",
    "                raw_tweet = tid_to_raw_tweet[tid].encode('utf8', 'replace').replace(b\"\\n\", b' ').replace(b\"\\t\", b' ')\n",
    "                raw_tweet = re.sub(b'((www\\.[^\\s]+)|(https?://[^\\s]+)|(pic\\.twitter\\.com/[^\\s]+))','', raw_tweet)\n",
    "                selected_raw_tweets_set.add(raw_tweet.decode('utf8', 'ignore').strip())\n",
    "                tids_list.append(tid)\n",
    "                #if tid_to_urls_window_corpus[tid]:\n",
    "                #    urls_list.append(tid_to_urls_window_corpus[tid])\n",
    "                for id in cluster_to_tids[headline_to_cluster[headline_corpus[i]]]:\n",
    "                    tids_cluster.append(id)\n",
    "            raw_headline = tid_to_raw_tweet[headline_to_tid[headline_corpus[hclidx[0]]]]\n",
    "            raw_headline = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+)|(pic\\.twitter\\.com/[^\\s]+))','', raw_headline)\n",
    "            raw_headline = raw_headline.encode('utf8', 'replace').replace(b\"\\n\", b' ').replace(b\"\\t\", b' ')\n",
    "            keywords_list = str(sorted(list(set(keywords[:-1].split(\",\")))))[1:-1].encode('utf8', 'replace').replace(b'u\\'',b'').replace(b'\\'',b'')\t\n",
    "\n",
    "            for tid in tids_cluster:\n",
    "                if len(urls_list) < 1 and tid not in tids_set:\n",
    "                        raw_tweet = tid_to_raw_tweet[tid].encode('utf8', 'replace').replace(b\"\\n\", b' ').replace(b\"\\t\", b' ')\n",
    "                        raw_tweet = re.sub(b'((www\\.[^\\s]+)|(https?://[^\\s]+)|(pic\\.twitter\\.com/[^\\s]+))',b'', raw_tweet)\n",
    "                        raw_tweet = raw_tweet.decode('utf8', 'ignore')\n",
    "                        if raw_tweet.strip() not in selected_raw_tweets_set:\n",
    "                            tids_list.append(tid)\n",
    "                            #urls_list.append(tid_to_urls_window_corpus[tid])\n",
    "                            selected_raw_tweets_set.add(raw_tweet.strip())\n",
    "            try:\n",
    "                print(\"\\n\", clean_headline.decode('utf8', 'ignore'))\n",
    "            except:pass\n",
    "            urls_set = set()\n",
    "            for url_list in urls_list:\n",
    "                for url in url_list:\n",
    "                    urls_set.add(url)\n",
    "            dtime = tweet_old_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "            fout.write(\"\\n\" + str(dtime) + \"\\t\" + raw_headline.decode('utf8', 'ignore') + \"\\t\" + keywords_list.decode('utf8', 'ignore') + \"\\t\" + str(tids_list)[1:-1] + \"\\t\" + str(list(urls_set))[1:-1][2:-1].replace('\\'','').replace('uhttp','http'))\n",
    "        window_corpus = []\n",
    "        tids_window_corpus = []\n",
    "        tid_to_urls_window_corpus = {}\n",
    "        tid_to_raw_tweet = {}\n",
    "        ntweets = 0\n",
    "        if t == 4:\n",
    "            dfVocTimeWindows = {}\n",
    "            t = 0\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet_old_time.strftime('%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "headline_vectorizer = CountVectorizer(tokenizer=custom_tokenize_text, binary=True, min_df=1, ngram_range=(1,1))\n",
    "H = headline_vectorizer.fit_transform(headline_corpus)\n",
    "print(\"H.shape:\", H.shape)\n",
    "vocH = headline_vectorizer.get_feature_names()\n",
    "\n",
    "Hdense = np.matrix(H.todense()).astype('float')\n",
    "distH = pairwise_distances(Hdense, metric='cosine')\n",
    "#\t\t\t\tprint \"fastcluster, avg, euclid\"\n",
    "HL = fastcluster.linkage(distH, method='average')\n",
    "dtH = 1.0\n",
    "indHL = sch.fcluster(HL, dtH*distH.max(), 'distance')\n",
    "freqHCl = Counter(indHL)\n",
    "print(\"hclust cut threshold:\", dtH)\n",
    "print(\"n_clusters:\", len(freqHCl))\n",
    "print(freqHCl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
